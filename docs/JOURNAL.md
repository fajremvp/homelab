# Di√°rio de Bordo

Este arquivo documenta a jornada, erros, aprendizados e decis√µes di√°rias.
Para mudan√ßas estruturais formais, veja o [CHANGELOG](../CHANGELOG.md).

---
## 2026-01-28
**Status:** ‚úÖ Sucesso (Com alta complexidade resolvida)

**Foco:** Observabilidade Ativa (Alertas) e Monitoramento de Virtualiza√ß√£o (Proxmox/LXC).

- **CrowdSec (Corre√ß√£o Cr√≠tica):**
    - **Sintoma:** Container CrowdSec em loop de erro DNS (`connection refused` para `127.0.0.53`).
    - **Causa:** O container herdava o `/etc/resolv.conf` do Host (systemd-resolved), mas n√£o tinha acesso ao loopback do host.
    - **Solu√ß√£o:** For√ßado DNS expl√≠cito (`10.10.30.5`, `1.1.1.1`) no `docker-compose.yml`. Comunica√ß√£o com a CAPI e Bouncer restabelecida.

- **Alertmanager & Ntfy (Observabilidade Ativa):**
    - Implementado `alert.rules.yml` no Prometheus (Regras: InstanceDown, DiskSpace, HighRAM, HighCPU).
    - Configurado Alertmanager para enviar notifica√ß√µes JSON via Webhook para o Ntfy local (`deny-all` com Token).
    - **Troubleshooting:**
        - Erro de permiss√£o (`0600`) no arquivo de config gerado pelo Ansible impedia leitura pelo usu√°rio `nobody` do container. Ajustado para `0644`.
        - Erro de volume: O arquivo de regras n√£o estava mapeado no `docker-compose`. Corrigido.
    - **Teste:** Exeutado `systemctl stop prometheus-node-exporter`, ap√≥s cerca de 4 minutos foi recebido o alerta no ntfy.

- **Expans√£o de Agentes (Node Exporter):**
    - Instalado `prometheus-node-exporter` nativo no Host F√≠sico (Proxmox) e na VM Vault.
    - **Network:** Ajustada regra UFW no Vault para permitir entrada na porta 9100 apenas vinda do DockerHost (`10.10.30.10`).

- **Proxmox VE Exporter (O Desafio do Dia):**
    - **Objetivo:** Monitorar m√©tricas individuais de LXCs e VMs (que o Node Exporter n√£o v√™).
    - **Incidente (Dependency Hell):** A imagem `prompve/prometheus-pve-exporter:latest` cont√©m uma vers√£o da biblioteca `proxmoxer` incompat√≠vel com os par√¢metros `token` ou `api_token` do script de inicializa√ß√£o. Causou *crash loop*.
    - **Workaround:** Revertido m√©todo de autentica√ß√£o para `user/password` no `pve.yml`.
    - **Alertas:** Criadas regras inteligentes usando `rate()` para CPU de VMs, evitando falsos positivos.

- **Grafana as Code:**
    - Dashboard ID 10347 (Proxmox VE) importado, higienizado (remo√ß√£o de IDs fixos) e salvo como c√≥digo em `provisioning/dashboards/proxmox-ve.json` para persist√™ncia via Ansible.

## 2026-01-27
**Status:** ‚ùå Falha (Experimento Abortado)

**Foco:** Implementa√ß√£o de IA Local (RAG Assistant) e Benchmark de Performance CPU-Only.

- **Objetivo:** Criar um assistente "Jarvis" soberano (Ollama + Open WebUI) rodando no hardware existente (i5-12400 + 64GB RAM) capaz de ler a documenta√ß√£o do Homelab (RAG).
- **E o Clawdbot?** √â uma ferramenta de agente aut√¥nomo. Ele executa coisas. Para ele ser √∫til, ele precisa de permiss√£o de escrita e execu√ß√£o. No meu Homelab focado em seguran√ßa ("Default Deny"), instalar um agente que varre o sistema e tem acesso ao shell √© pedir para ser hackeado ou sofrer um acidente catastr√≥fico (ex: alucina√ß√£o de IA deletando configs ou vazar dados). √â "hype" de X, n√£o infraestrutura s√©ria. Talvez esperar o hype abaixar, ver o que a comunidade est√° achando e implementar com cuidados no futuro.

- **Infraestrutura Provisionada:**
    - Criado LXC `110 (AI-Node)` na VLAN 30 com 24GB de RAM dedicados e 4 vCores.
    - Automa√ß√£o via Ansible: Playbook `setup_ai_node.yml` implementado para deploy da stack Docker + Clonagem do Reposit√≥rio para contexto.
    - **Corre√ß√£o de Runtime:** Necess√°rio remover limites de `ulimit/memlock` do Docker Compose, pois containers LXC n√£o permitem controle direto de mem√≥ria do Kernel do Host.

- **Benchmark de Modelos (CPU Inference):**
    - **Teste 1: Cohere Command-R (35B):**
        - *Expectativa:* Alta capacidade de RAG e cita√ß√µes precisas.
        - *Realidade:* Invi√°vel. O modelo de ~20GB saturou a banda de mem√≥ria DDR4. Lat√™ncia de resposta superior a 6 minutos.
    - **Teste 2: Llama 3.1 (8B Instruct):**
        - *Expectativa:* Modelo equilibrado padr√£o de mercado.
        - *Realidade:* Gera√ß√£o lenta (~3-5 tokens/s). A experi√™ncia de chat em tempo real foi frustrante e "travada".
    - **Teste 3: Llama 3.2 (3B):**
        - *Expectativa:* Modelo "Edge" otimizado para lat√™ncia baixa.
        - *Realidade:* Melhor velocidade, mas ainda aqu√©m da instantaneidade necess√°ria para um assistente fluido. A intelig√™ncia reduzida tamb√©m comprometeu a an√°lise de documentos complexos.

- **Veredito T√©cnico:**
    - A infer√™ncia de LLMs modernos depende criticamente de largura de banda de mem√≥ria (VRAM/RAM) e processamento paralelo massivo (Cores CUDA).
    - O Intel i5-12400 (mesmo com AVX2) n√£o possui throughput suficiente para sustentar uma experi√™ncia de chat agrad√°vel sem GPU dedicada.

- **A√ß√£o de Conten√ß√£o (Cleanup):**
    - **Infraestrutura:** Container LXC 110 destru√≠do e recursos (24GB RAM) devolvidos ao Host.
    - **C√≥digo:** Revertidos commits de infraestrutura (`hosts.ini`, playbooks) para manter o reposit√≥rio limpo de "c√≥digo morto".
    - **Futuro:** Projeto suspenso at√© a aquisi√ß√£o de acelerador de hardware (GPU Nvidia ou NPU dedicada).
## 2026-01-25
**Status:** ‚úÖ Sucesso (Security Incident Response & Hardening)

**Foco:** Resposta a Incidente de Vazamento de Credenciais, Refatora√ß√£o do Vault e Observabilidade do CrowdSec.

- **CrowdSec Observability (M√©tricas & Alertas):**
    - **Prometheus:** Realizada "cirurgia" no `config.yaml` dentro do container para habilitar o m√≥dulo Prometheus e alterar o bind para `0.0.0.0`, permitindo coleta externa na porta `6060`.
    - **Ntfy Integration:**
        - Implementado template de notifica√ß√£o `http.yaml`.
        - **Fix de Template:** Simplificado o formato da mensagem para remover a vari√°vel `.Source.CN` (Country Name), que causava crash do plugin em testes manuais (IPs sem geolocaliza√ß√£o).
        - **Fix de Rede:** Alterada a URL de notifica√ß√£o de `http://10.10.30.10` para `http://ntfy:80` (Rede interna Docker) para contornar problemas de *Hairpin NAT* e erros de certificado SSL autoassinado.
    - **Valida√ß√£o:** Testes de ataque simulado (`cscli decisions add`) geram alertas imediatos no celular.

- **Incidente de Seguran√ßa (Data Leak):**
    - **Evento:** Durante o push das configura√ß√µes de notifica√ß√£o, identificou-se que o Token do Ntfy e os `ROLE_ID` do Vault (Authentik/Vaultwarden) foram commitados em texto plano no reposit√≥rio p√∫blico.
    - **An√°lise de Risco:** Exposi√ß√£o de credenciais de "Nome de Usu√°rio" (RoleID) e Token de Push. Risco de spam de notifica√ß√µes e redu√ß√£o da entropia de seguran√ßa do Vault.
    - **A√ß√£o Imediata:** Revoga√ß√£o do Token Ntfy e desabilita√ß√£o/habilita√ß√£o do m√©todo AppRole no Vault, invalidando todos os IDs anteriores.

- **Refatora√ß√£o Arquitetural (Vault AppRole):**
    - **Nova Estrat√©gia:** Adotado o padr√£o "Gold Standard" para reposit√≥rios p√∫blicos.
        - Scripts de inicializa√ß√£o (`start-with-vault.sh`) transformados em arquivos "burros" que leem credenciais do disco.
        - Segredos (`ROLE_ID`, `SECRET_ID`) movidos para `/etc/vault/` com permiss√£o `0600` (root only).
    - **Automa√ß√£o Ansible:**
        - Atualizado `manage_stacks.yml` para solicitar as novas credenciais via `vars_prompt` (RAM apenas) e grav√°-las nos arquivos protegidos.
        - Templates `.j2` removidos do fluxo de c√≥pia direta.
    - **Limpeza:** Removidos arquivos sens√≠veis do hist√≥rico Git e aplicados novos templates sanitizados.

- **Corre√ß√£o de Backup (Disaster Recovery):**
    - **Gap Identificado:** Os diret√≥rios `/opt/security` (Dados do CrowdSec) e a nova estrutura `/etc/vault` (Credenciais de Boot) n√£o estavam no backup di√°rio.
    - **Fix:** Atualizado playbook `setup_backup.yml` para incluir estes caminhos.
    - **Valida√ß√£o:** Execu√ß√£o manual do Restic confirmou a inclus√£o dos arquivos `.secretid` e `.roleid` no snapshot criptografado.

- **Dashboard as Code (Grafana):**
    - **Implementa√ß√£o:** Baixado o JSON oficial do CrowdSec (ID 19010) para o reposit√≥rio Git.
    - **Incidente de Provisionamento:** O dashboard carregava vazio ("Datasource not found").
    - **Diagn√≥stico:** O Grafana em modo *provisioning* n√£o resolve o nome "Prometheus" automaticamente se o JSON esperar um Input vari√°vel.
    - **Corre√ß√£o S√™nior:** Hardcoded o UID do Datasource (`dfa44v3b15a80b`) diretamente no JSON antes do commit, eliminando a depend√™ncia de inputs manuais.

- **Nobreak NHS Gamer Play (Incompatibilidade):**
    - **Tentativa:** Integra√ß√£o via NUT no Raspberry Pi (USB).
    - **Hardware ID:** `0925:1241` (NXP/Lakeview Virtual COM).
    - **Diagn√≥stico:** - Driver `nutdrv_qx`: Falha (Dispositivo n√£o √© HID compliant).
        - Driver `blazer_ser`: Falha (Protocolo propriet√°rio/Short Reply na porta `/dev/ttyACM0`).
    - **Conclus√£o:** O modelo possui firmware travado/propriet√°rio incompat√≠vel com o padr√£o open-source.
    - **A√ß√£o:** Devolu√ß√£o e encontrar um outro, que seja compat√≠vel.

- **Status Final:**
    - Infraestrutura recuperada e mais segura do que antes do incidente.
    - Servi√ßos Authentik e Vaultwarden reiniciados e operando com as novas credenciais rotacionadas.
    - CrowdSec com uma boa observabilidade no Grafana.
    - Reposit√≥rio Git limpo de segredos.
## 2026-01-24
**Status:** ‚ö†Ô∏è Sucesso Parcial (Per√≠metro OK, Camada 7 Parcial)

**Foco:** Carregamento do Nobreak NHS, Deploy do CrowdSec (LAPI + Bouncer) e Troubleshooting de Parsing de Camada 7.

- **Infraestrutura El√©trica (Nobreak NHS):**
    - **Hardware:** Adquirido Nobreak NHS Gamer Play 1000VA (Senoidal Pura).
    - **Protocolo de Ativa√ß√£o:** Iniciado ciclo de carga de 12 horas (sem carga conectada) para equaliza√ß√£o das baterias internas (2x 7Ah).
    - **Dimensionamento:** Carga estimada de 160W (~26%), garantindo autonomia superior a 20 minutos.

- **Implementa√ß√£o CrowdSec (Defesa Ativa):**
    - **Arquitetura C√©rebro-M√∫sculo:** LAPI (Agente/C√©rebro) centralizado no DockerHost e Bouncer (M√∫sculo) no OPNsense.
    - **Seguran√ßa de Rede:** Porta 8080 do LAPI configurada com *Bind IP* exclusivo para o IP interno do DockerHost (`10.10.30.10`), isolando a API da rede externa.
    - **Resolu√ß√£o de Metadados:** Conex√£o do CrowdSec ao `socket-proxy` via `DOCKER_HOST` para identifica√ß√£o de nomes de containers nos logs.

- **Troubleshooting de Parsing (Authentik):**
    - **Desafio do Hub:** A cole√ß√£o oficial para Authentik foi identificada como `firix/authentik`.
    - **YAML Hell (acquis.yaml):** - *Tentativa 1 (Falha):* Filtros din√¢micos via `evt.Parsed` falharam (aquisi√ß√£o ocorre antes do parsing).
        - *Tentativa 2 (Sucesso):* Implementado apontamento via **Hardcoded Container ID** no `acquis.yaml` para for√ßar o `type: authentik`.
        - **‚ö†Ô∏è Manuten√ß√£o Cr√≠tica:** Caso o container do Authentik seja recriado (update), o ID em `acquis.yaml` deve ser atualizado para evitar cegueira do parser.
    - **Resultado T√©cnico Real:** 
        - O parser `firix/authentik-logs` est√° ativo e recebendo eventos (`Hits > 0`).
        - **Parsed = 0** mesmo ap√≥s falhas reais de login.
        - **Impacto:** Nenhuma decis√£o autom√°tica de banimento √© gerada a partir de falhas de autentica√ß√£o no Authentik.
        - **Estado Atual:** Monitoramento funcional, **remedia√ß√£o inativa** para Authentik.
    - **Causa Raiz (Root Cause):**
        - A cole√ß√£o `firix/authentik` utiliza Regex compat√≠vel com vers√µes anteriores do Authentik.
        - O Authentik 2025 alterou o formato dos eventos `login_failed`, impedindo a extra√ß√£o de IP (`source_ip`).
        - **Conclus√£o:** Limita√ß√£o da cole√ß√£o da comunidade, n√£o da infraestrutura local.


- **Integra√ß√£o OPNsense (Bouncer):**
    - **Plugin `os-crowdsec`:** Superada falha de valida√ß√£o da GUI (que exige campos locais mesmo para LAPI remota) usando configura√ß√£o "fake" (127.0.0.1) e edi√ß√£o manual do `/usr/local/etc/crowdsec/bouncers/crowdsec-firewall-bouncer.yaml` via SSH.
    - **Valida√ß√£o:** Teste com IP `1.1.1.1` resultou em bloqueio imediato na tabela `crowdsec_blocklists`.
## 2026-01-22
**Status:** ‚úÖ Sucesso (Observability Repair & GitOps Level 2)

**Foco:** Corre√ß√£o de M√©tricas do Traefik, Ressurrei√ß√£o do Loki (Config V3) e Implementa√ß√£o de Dashboard as Code.

- **Corre√ß√£o de M√©tricas (Traefik v3):**
    - **Sintoma:** O Dashboard do Traefik no Grafana n√£o exibia dados ("No Data"), apesar da porta 8082 estar exposta.
    - **Diagn√≥stico:** O Traefik estava gerando m√©tricas, mas n√£o estava vinculado ao EntryPoint dedicado. O endpoint `/metrics` retornava 404.
    - **Corre√ß√£o:** Adicionado `--metrics.prometheus.entryPoint=metrics` no `docker-compose.yml`.
    - **Valida√ß√£o:** `curl http://10.10.30.10:8082/metrics` passou a retornar o payload do Prometheus.
    - **Aprendizado:** Grafana vazio muitas vezes √© apenas o *Time Range* errado. Alterado de "Last 6 hours" para "Last 5 minutes" para visualizar dados recentes.

- **Troubleshooting do Loki (Crash Loop):**
    - **Incidente:** O Grafana exibia erro `Live tailing was stopped... undefined` e o container do Loki reiniciava a cada 10 segundos.
    - **Causa Raiz (Deprecia√ß√£o):** O arquivo de configura√ß√£o `local-config.yaml` utilizava par√¢metros da vers√£o 2.x incompat√≠veis com a imagem `loki:3.6.3`.
    - **Corre√ß√µes Aplicadas:**
        1.  **Shared Store:** Removida a linha `shared_store: filesystem` (o Loki v3 infere isso automaticamente).
        2.  **Compactor:** Adicionado `delete_request_store: filesystem` no bloco do `compactor` (Obrigat√≥rio quando `retention_enabled` √© true).
    - **Recupera√ß√£o do Agente:** O container `alloy` (coletor) havia desistido de enviar logs durante a falha. Um `docker compose restart alloy` restabeleceu o fluxo de logs para o Grafana.

- **Implementa√ß√£o de Dashboard as Code (Imutabilidade):**
    - **Objetivo:** Eliminar o "ClickOps". Dashboards devem ser arquivos no Git, n√£o configura√ß√µes manuais no banco de dados.
    - **Arquitetura:**
        - Criada estrutura separada: `provisioning/dashboards` (Configura√ß√£o do Provider) e `dashboards/` (Arquivos JSON).
        - Mapeados volumes no `docker-compose.yml` do Grafana.
    - **Desafio de Deploy (Ansible):**
        - *Erro 1:* Execu√ß√£o do Ansible fora da raiz (`/opt/homelab`), causando falha na leitura do `ansible.cfg` e invent√°rio.
        - *Erro 2:* Estrutura de pastas inconsistente no reposit√≥rio de origem (Arch Linux), misturando JSONs com configs YAML.
    - **Solu√ß√£o:** Reorganiza√ß√£o das pastas no Git local (`mv *.json dashboards/`) e execu√ß√£o correta do Ansible.
    - **Resultado:** Dashboards marcados como "Provisioned". O Grafana agora impede a exclus√£o manual ("Cannot be deleted"), garantindo integridade da infraestrutura.

- **Conceitos Adotados/Aprendidos:**
    - **M√©todo U.S.E. (Utilization, Saturation, Errors):** Aplicado para an√°lise de Hardware (Node Exporter).
    - **M√©todo R.E.D. (Rate, Errors, Duration):** Aplicado para an√°lise de Servi√ßos.
## 2026-01-19
**Status:** ‚úÖ Sucesso (DNS High Availability)

**Foco:** Implementa√ß√£o de Redund√¢ncia de DNS, Hardening Forense e Corre√ß√£o de Roteamento.

- **Implementa√ß√£o do DNS Secund√°rio (Raspberry Pi):**
    - **Objetivo:** Garantir resolu√ß√£o de nomes mesmo se o LXC Alpine falhar.
    - **Deploy:** Criado playbook `setup_rpi_adguard.yml` instalando AdGuard Home v0.107.56.
    - **Desafio de Sintaxe (YAML Hell):**
        - *Sintoma:* O servi√ßo entrava em loop de rein√≠cio com erro `cannot unmarshal !!seq into string`.
        - *Causa:* O bin√°rio do AdGuard √© estrito com indenta√ß√£o e tipos (Lista vs String) no arquivo de configura√ß√£o, especialmente nas chaves `bind_hosts`.
        - *Solu√ß√£o:* Ado√ß√£o de sintaxe YAML Inline (ex: `bind_hosts: [ "0.0.0.0" ]`) e defini√ß√£o expl√≠cita de `schema_version: 29` no template Ansible.
    - **Desafio de Valida√ß√£o (Init Stats):**
        - *Erro:* `fatal: init stats: unsupported interval: less than an hour`.
        - *Solu√ß√£o:* Mesmo com estat√≠sticas desativadas (`enabled: false`), o validador exige um intervalo v√°lido. Configurado `interval: 24h` para satisfazer o check, mantendo a coleta desligada.

- **Hardening Forense (Zero Footprint):**
    - **Arquitetura:** O Raspberry Pi foi configurado para **n√£o persistir** nenhum dado de navega√ß√£o no cart√£o SD.
    - **RAM Disk (Tmpfs):** O diret√≥rio de dados (`/opt/AdGuardHome/data`) √© montado em RAM.
    - **Permiss√µes Estritas:** Configurado `mode=0700` no mount point.
        - *Valida√ß√£o:* `df -h` confirma `tmpfs`, e acesso via usu√°rio comum retorna `Permission denied`. Apenas root acessa a mem√≥ria do processo.
    - **Logs:** `querylog` e `statistics` desativados na configura√ß√£o. `journald` silenciado via `StandardOutput=null` no Systemd.

- **Corre√ß√£o de Infraestrutura de Rede (OPNsense):**
    - **Incidente:** O Arch Linux (VLAN Trusted) conseguia pingar o Gateway, mas falhava ao acessar a internet (`Destination Host Unreachable` para 1.1.1.1).
    - **Diagn√≥stico:** O campo **Gateway** no escopo DHCPv4 da interface Trusted estava definido como `None`. Os clientes recebiam IP mas n√£o rota default.
    - **Corre√ß√£o:** Definido Gateway para `10.10.20.1` (IP do OPNsense na VLAN). Conectividade restaurada imediatamente.
    - **Ajuste de DNS System:** Removidos gateways associados aos DNS Servers em *System > Settings > General*, corrigindo o erro `You can not assign a gateway to DNS server which is on a directly connected network`.

- **Teste de Failover (Chaos Engineering):**
    - **Cen√°rio:** Container do AdGuard Prim√°rio (`10.10.30.5`) desligado intencionalmente.
    - **Resultado:**
        1. O cliente (Arch) detectou timeout no prim√°rio.
        2. Automaticamente chaveou para o secund√°rio (`192.168.0.5`).
        3. `dig google.com` confirmou resposta vinda do Pi.
        4. Navega√ß√£o continuou fluida.
    - **Conclus√£o:** A redund√¢ncia de DNS est√° operante e transparente.
## 2026-01-18
**Status:** ‚úÖ Sucesso (Hardening & Edge Observability)

**Foco:** Seguran√ßa do Raspberry Pi e Integra√ß√£o com Prometheus Central

- **Hardening do Raspberry Pi (Management Node):**
    - **Integra√ß√£o Ansible:** Adicionado grupo `[rpi]` ao invent√°rio e configurada troca de chaves SSH com o controlador.
    - **Playbook Dedicado:** Criado `hardening_rpi.yml`, derivado do padr√£o Debian, mas adaptado para hardware f√≠sico.
        - *Ajuste T√°tico:* Removido pacote `libraspberrypi-bin` que n√£o est√° dispon√≠vel nos reposit√≥rios padr√£o do Debian 13 (Trixie), evitando falha de provisionamento.
    - **Resultados:**
        - SSH configurado para aceitar **apenas chaves** (Senha removida).
        - Fail2Ban ativo protegendo a porta 22 contra ataques na rede interna/VPN.
        - Timezone sincronizado para `America/Sao_Paulo`.

- **Expans√£o da Observabilidade (Prometheus):**
    - **Agente:** Instalado `prometheus-node-exporter` no Raspberry Pi via Ansible.
    - **Coleta (Scrape):** Configurado Prometheus no DockerHost para ler m√©tricas do Pi (`192.168.0.5:9100`).
    - **Troubleshooting (Config Reload):**
        - *Sintoma:* O Ansible atualizou o arquivo `prometheus.yml` no DockerHost, mas o Grafana n√£o mostrava os dados.
        - *Causa:* O servi√ßo Prometheus dentro do container n√£o recarregou a configura√ß√£o automaticamente apenas com a mudan√ßa do arquivo.
        - *Solu√ß√£o:* Executado `docker restart prometheus`.
    - **Valida√ß√£o:** Query `up{job="rpi-edge"}` retornou `1` no Grafana. O Pi agora √© observ√°vel (CPU, RAM, Disco, Temperatura).
## 2026-01-17
**Status:** üîÑ Pivotagem de Hardware (UPS)

**Foco:** Engenharia Reversa do Protocolo do Nobreak e Decis√£o de Devolu√ß√£o.

- **Diagn√≥stico Profundo do Nobreak (Ragtech M2):**
    - **Identifica√ß√£o:** Chipset Microchip detectado (`ID 04d8:000a`). Interface serial emulada em `/dev/ttyACM0`.
    - **Tentativas de Driver (NUT):**
        - `nutdrv_qx`: Testados dialetos `megatec`, `krauler` e `voltronic`. Resultado: `Device not supported`.
        - `blazer_ser`: Testadas velocidades 2400, 9600 e 460800 baud. Resultado: Timeout/No supported UPS detected.
    - **Aut√≥psia (Python Script):**
        - Criado script para envio de comandos brutos (Raw Serial) com sinal DTR/RTS for√ßado.
        - **Resultado:** O dispositivo respondeu com o byte `\xca` (Hex 202) para qualquer comando padr√£o ASCII (`Q1`, `I`).
    - **Conclus√£o T√©cnica:** A Ragtech implementou um protocolo bin√°rio propriet√°rio/fechado neste lote de chips, incompat√≠vel com os padr√µes abertos (Megatec/Voltronic) utilizados pelo NUT.

- **Decis√£o de Neg√≥cios:**
    - O uso de scripts de terceiros ("gambiarras" em Python) para traduzir o protocolo foi considerado, mas descartado por violar o princ√≠pio de confiabilidade para infraestrutura cr√≠tica.
    - **A√ß√£o:** Iniciado processo de devolu√ß√£o do produto por arrependimento.
    - **Pr√≥ximos Passos:** Aquisi√ß√£o de um novo Nobreak (APC ou NHS) com compatibilidade nativa Linux comprovada.

- **Limpeza do Raspberry Pi:**
    - Removidos pacotes de diagn√≥stico (`python3-serial`, `nut-client`).
    - Removidas regras Udev e configura√ß√µes do NUT.
    - O Pi permanece operante como n√≥ de gerenciamento, aguardando o novo UPS.

## 2026-01-16
**Status:** ‚úÖ Sucesso (Recupera√ß√£o do Management Node)

**Foco:** Reinstala√ß√£o do Raspberry Pi, Corre√ß√£o de I/O e Configura√ß√£o de RTC.

- **Recupera√ß√£o do Raspberry Pi (OS & Storage):**
    - **Problema:** Boot loop e erros de I/O (`uas_eh_device_reset_handler`) persistiam mesmo com a nova fonte.
    - **Causa Raiz:** Incompatibilidade do driver UAS (USB Attached SCSI) do Kernel Linux com o controlador JMicron (`152d:0583`) do case SSD.
    - **Solu√ß√£o (Quirks):** Adicionado `usb-storage.quirks=152d:0583:u` ao `/boot/cmdline.txt`, for√ßando o modo "Bulk-Only Transport" (mais lento, por√©m est√°vel).
    - **Resultado:** Sistema est√°vel, boot r√°pido e zero erros de I/O.

- **Configura√ß√£o de Rede (Debian 13/Bookworm):**
    - Abandonado `dhcpcd` (obsoleto). Configurado IP Est√°tico `192.168.0.5` utilizando **NetworkManager** (`nmcli`).

- **Rel√≥gio de Hardware (RTC DS3231):**
    - **Desafio:** O Debian 13 mudou a localiza√ß√£o dos arquivos de configura√ß√£o e removeu scripts antigos de hwclock.
    - **Implementa√ß√£o:**
        1. Ativado I2C via `raspi-config`.
        2. Adicionado overlay `dtoverlay=i2c-rtc,ds3231` em `/boot/firmware/config.txt`.
        3. Removido pacote `fake-hwclock` para evitar conflitos.
        4. Sincroniza√ß√£o realizada via `hwclock -w`.
    - **Valida√ß√£o:** `hwclock -r` retorna a data correta persistente, garantindo logs precisos mesmo sem internet.
## 2026-01-15
**Status:** ‚è∏Ô∏è Pausa For√ßada (Hardware Bloqueante)

**Foco:** Provisionamento do Raspberry Pi, Teste de Carga do Nobreak e Gest√£o de Crise de Hardware.

- **Incidente El√©trico (Nobreak):**
    - **A√ß√£o:** (Agi sem pensar) Realizado teste de carga conectando uma chaleira el√©trica (~1850W) nas tomadas do Nobreak Ragtech.
    - **Resultado:** O equipamento entrou em estado de alarme imediato (Bip cont√≠nuo/r√°pido), indicando **Sobrecarga (Overload)**.
    - **Diagn√≥stico:** A pot√™ncia da carga resistiva excedeu largamente a capacidade nominal (840W) do inversor.
    - **Corre√ß√£o:** Carga removida. Nobreak conectado √† rede el√©trica sem dispositivos de sa√≠da para ciclo de carga inicial de 24 horas (recomenda√ß√£o do manual).

- **Provisionamento do Pi (Software):**
    - Instalado `rpi-imager` no Arch Linux.
    - Gravada imagem **Raspberry Pi OS Lite (64-bit)** no SSD via USB 3.0.
    - **Configura√ß√£o Headless:** Definido hostname `rpi`, usu√°rio `fajre` e SSH habilitado via configura√ß√µes avan√ßadas do Imager.
    - Excelente programa, btw.

- **Incidente de Suprimentos (Fonte do Pi):**
    - A fonte adquirida ("Kit Gamer U1002") chegou com conector incompat√≠vel (P4/Micro-B em vez de USB-C). Devolu√ß√£o iniciada.
    - **Workaround Falho:** Tentativa de boot utilizando carregador de celular (Xiaomi).
    - **Sintoma:** O Pi ligou, mas o monitor exibiu erros de I/O c√≠clicos: `scsi host0: uas_eh_device_reset_handler`.
    - **Causa Raiz:** **Brownout**. O carregador n√£o suportou o pico de corrente exigido pelo SSD via USB 3.0, causando queda de tens√£o e desligamento do controlador de disco.
    - **A√ß√£o:** Comprada fonte **CanaKit 3.5A** (Padr√£o oficial) com filtro de ru√≠do. Instala√ß√£o suspensa at√© a chegada (Sexta-feira, 16/01).

- **Decis√£o Arquitetural (Seguran√ßa):**
    - Formalizada a decis√£o de **N√ÉO utilizar criptografia LUKS** no Raspberry Pi.
    - **Justificativa:** O Pi √© um dispositivo de recupera√ß√£o de desastres. Exigir senha de boot criaria um deadlock ("Ovo e Galinha") onde o dispositivo necess√°rio para liberar o acesso remoto estaria ele mesmo inacess√≠vel, e tamb√©m n√£o h√° nada t√£o sens√≠vel para esconder (Split Storage, ver melhor a explica√ß√£o em docs/services/rpi.md Seguran√ßa ser√° garantida por isolamento de rede e ACLs na VPN.
## 2026-01-14
**Status:** ‚úÖ Sucesso (Observability Phase 1 & PKI Pivot)

**Foco:** Implementa√ß√£o do N√∫cleo de Observabilidade, Pivotagem de PKI e Hardening de Rede.

- **Arquitetura de Observabilidade (LGM Stack):**
    - Implantado stack central no DockerHost via Ansible:
        - **Prometheus (v3.9):** Scrape local (15 dias de reten√ß√£o).
        - **Loki (v3.6):** Recebendo logs. Configurado `max_streams_per_user` para evitar OOM.
        - **Grafana (v12.3):** Autentica√ß√£o delegada ao Authentik (ForwardAuth).
        - **Alloy:** Agente unificado. L√™ logs do host via `journald` e containers via arquivos `json-file`.
        - **Ntfy:** Gateway de notifica√ß√µes push (Self-hosted).
    - **Docker Logging:** Driver alterado globalmente para `json-file` (rota√ß√£o 3x10MB) para permitir leitura direta de disco pelo Alloy, reduzindo overhead no daemon.

- **Pivotagem de PKI (SSL/TLS):**
    - **Erro Conceitual:** Assumiu-se inicialmente que o Traefik gerenciava uma PKI interna (Step-CA). Os logs revelaram o uso de "Default Certs" autoassinados, rejeitados pelo Android.
    - **Solu√ß√£o Pragm√°tica:** Implementada CA Local via `mkcert` (Trust-on-device).
        - Gerado certificado Wildcard `*.home` e IP SAN `10.10.30.10`.
        - **Security Decision:** Chaves privadas (`.key`) transferidas via SCP (Out-of-band), estritamente fora do Git.
        - **Trust:** `rootCA.pem` instalada manualmente no Android e Arch Linux.

- **Resolu√ß√£o de Roteamento (Traefik 504 Timeout):**
    - **Incidente:** Gateway Timeout ao acessar Ntfy via Ingress.
    - **Causa:** Ambival√™ncia de roteamento em containers multi-rede (`monitoring` vs `proxy`).
    - **Corre√ß√£o:** Fixada rede de sa√≠da via label `traefik.docker.network=proxy` e porta de servi√ßo expl√≠cita `loadbalancer.server.port=80`.

- **Hardening de Automa√ß√£o (Ansible):**
    - **Seguran√ßa:** Implementado `vars_prompt` para inser√ß√£o de segredos em runtime, evitando vazamento em hist√≥rico de shell.
    - **Depend√™ncias:** Adicionado `rsync` ao `hardening_debian.yml` para viabilizar m√≥dulo `synchronize`.
    - **Escopo:** Restrita a configura√ß√£o de Docker apenas ao grupo `dockerhost`, preservando a integridade da VM Vault (Pure Debian).

- **Backup:**
    - Diret√≥rio `/opt/monitoring` inclu√≠do na pol√≠tica de backup do Restic. Snapshot validado.
## 2026-01-11
**Status:** ‚úÖ Sucesso (Host Hardening & Defense in Depth)

**Foco:** Prote√ß√£o contra Brute-Force (Fail2Ban) e Refinamento de SSH

- **Hardening do Proxmox (Host F√≠sico):**
    - Criado playbook dedicado `hardening_proxmox.yml`.
    - **Prote√ß√£o Web UI:** Implementado Fail2Ban monitorando logs do `pvedaemon` e `pveproxy` (Regex duplo) para bloquear tentativas de login na porta 8006.
    - **Backend Otimizado:** Configurado para ler logs diretamente do `systemd/journald` em vez de arquivos de texto.
    - **SSH:** Configurado `PermitRootLogin prohibit-password` (Apenas Chave).
- **Hardening Debian (DockerHost & Vault):**
    - Refatorado playbook `hardening_debian.yml` para padr√µes de produ√ß√£o.
    - **Fail2Ban:** Configurado com `mode = aggressive` no SSH para detectar falhas de pr√©-autentica√ß√£o.
    - **Whitelist de Rede:** Adicionada regra `ignoreip` para a rede de Gest√£o (10.10.10.x) e Trusted (10.10.20.x), prevenindo que automa√ß√µes ou erros de digita√ß√£o causem auto-lockout.
    - **SSH Moderno:** Substitu√≠do par√¢metro legado `ChallengeResponseAuthentication` por `KbdInteractiveAuthentication no` (Padr√£o Debian 12+).
    - **Estabilidade:** Alterada pol√≠tica de atualiza√ß√£o de `dist-upgrade` para `safe-upgrade` para evitar remo√ß√£o acidental de pacotes cr√≠ticos.
- **Valida√ß√£o:**
    - Testes de conex√£o confirmaram que chaves SSH continuam funcionando.
    - Status do Fail2Ban validado em todos os n√≥s (`jail sshd` ativo e backend systemd carregado).
## 2026-01-10
**Status:** ‚úÖ Sucesso (Hardening & Optimization)

**Foco:** Rota√ß√£o de Credenciais, Otimiza√ß√£o de DNS e Corre√ß√£o de Custos de Backup

- **Rota√ß√£o de Credenciais (Security Sprint):**
    - Substitu√≠das todas as senhas fracas/compartilhadas por senhas √∫nicas.
    - **Escopo:** Proxmox Host, OPNsense, DockerHost, Vault VM, Management LXC, AdGuard LXC, AdGuard Home (servi√ßo) e Vaultwarden.
    - **Armazenamento:** Todas as credenciais salvas no Vaultwarden.
- **Corre√ß√£o de Provisionamento Alpine:**
    - Identificado que o servi√ßo SSH n√£o iniciava automaticamente ap√≥s instala√ß√£o via Ansible em containers Alpine (OpenRC).
    - **Fix:** Adicionada tarefa expl√≠cita `service: name=sshd state=started enabled=yes` no playbook `hardening_alpine.yml`.
- **Otimiza√ß√£o do AdGuard Home:**
    - **Performance:** Upstream DNS alterado para "Parallel Requests" (Quad9 + Cloudflare) e ativado "Optimistic Caching" para respostas instant√¢neas.
    - **Privacidade/Seguran√ßa:** Ativado DNSSEC e desabilitada resolu√ß√£o IPv6 (foco em estabilidade IPv4 na LAN).
    - **Bloqueio:** Adicionada lista `OISD Big` (famosa por zero false-positives) e ativada lista `AdAway`.
    - **Logs:** Reten√ß√£o reduzida para 7 dias (Query) e 7 dias (Stats) para privacidade e economia de disco.
- **Backblaze B2 (Cost Management):**
    - Ajustada pol√≠tica de ciclo de vida do bucket para `Keep only the last version of the file`.
    - **Justificativa:** O Restic j√° gerencia o versionamento e snapshots internamente. A configura√ß√£o padr√£o do B2 ("Keep all versions") manteria arquivos deletados pelo `prune` cobrando armazenamento eternamente.
## 2026-01-09
**Status:** ‚úÖ Sucesso (GitOps, Hardening & Disaster Recovery)

**Foco:** Transforma√ß√£o da infraestrutura em C√≥digo (IaC), Seguran√ßa e Implementa√ß√£o de Backup Criptografado

- **Migra√ß√£o para GitOps (DockerHost):**
    - **Ado√ß√£o de Infraestrutura:** Importadas configura√ß√µes reais (`/opt/services/*`) via SCP para o reposit√≥rio Git, padronizando a estrutura em `configuration/dockerhost/{servi√ßo}`.
    - **Automa√ß√£o (Ansible):** Criado playbook `manage_stacks.yml` atuando como "Fonte da Verdade".
    - **L√≥gica H√≠brida:** - Servi√ßos simples (Traefik, Whoami) iniciados via m√≥dulo Docker direto.
        - Servi√ßos cr√≠ticos (Authentik, Vaultwarden) migrados para **Systemd Units** (`.service`) para garantir a inje√ß√£o de segredos via script `start-with-vault.sh`.

- **Hardening e Seguran√ßa:**
    - **Segrega√ß√£o de OS:** Criados playbooks distintos: `hardening_debian.yml` (DockerHost, Vault) e `hardening_alpine.yml` (Management, AdGuard).
    - **Lockout Incident (Aprendizado):** - *Erro:* O script Alpine definiu `PermitRootLogin no`. Como o Ansible conecta como root, houve bloqueio de acesso ao AdGuard.
        - *Solu√ß√£o:* Acesso via Console Proxmox, altera√ß√£o manual para `prohibit-password` e corre√ß√£o definitiva no playbook.

- **Backup do Firewall (OPNsense):**
    - **Plugin:** Implementado `os-git-backup`.
    - **Fix de Compatibilidade:** Gerado par de chaves **RSA (PEM Legacy)** e ajustada URL para `ssh://github.com/...` para contornar rejei√ß√£o de chaves Ed25519 pelo plugin.
    - **Resultado:** Backup autom√°tico e versionado da configura√ß√£o XML para reposit√≥rio privado a cada altera√ß√£o.

- **Backup de Dados (Restic + Backblaze B2):**
    - **Arquitetura Distribu√≠da:** Cada host possui seu pr√≥prio reposit√≥rio isolado e criptografado no Bucket B2 (`b2:bucket:/host`).
    - **Controle de Acesso de Rede (OPNsense):**
        - Configurado **Schedule** `HorarioBackupVault` (03:59 - 04:30) com validade at√© o fim de 2026.
        - Criada regra de firewall na VLAN 40 permitindo sa√≠da de dados apenas nesta janela temporal, mantendo o Vault isolado (Air-gapped) no restante do dia.
    - **Vault Strategy:** Criada Policy espec√≠fica e Token peri√≥dico com **Auto-Renova√ß√£o** via script di√°rio. Snapshots (`raft-YYYYMMDD.snap`) s√£o gerados localmente antes do upload.
    - **Automa√ß√£o:** Playbook `setup_backup.yml` auditado e Cronjobs distribu√≠dos para evitar gargalo de rede.

- **Disaster Recovery (Fire Drill):**
    - **Simula√ß√£o:** Arquivo `docker-compose.yml` do servi√ßo `whoami` deletado intencionalmente no DockerHost.
    - **Execu√ß√£o:**
        - *Falha Inicial:* Uso de `sudo` dropou as vari√°veis de ambiente do Restic.
        - *Corre√ß√£o:* Execu√ß√£o como root nativo carregando `source /etc/restic-env.sh`.
        - Comando: `restic restore <snapshot_id> --target / --include ...`
    - **Resultado:** Arquivo recuperado com sucesso, permiss√µes mantidas. Backup validado.
- **Corre√ß√£o de Timezone (Sincroniza√ß√£o de Rel√≥gios):**
    - Identificada discrep√¢ncia de hor√°rios entre Hosts (EST/UTC) e Proxmox (-03).
    - **A√ß√£o:** Integrada a corre√ß√£o diretamente nos playbooks de hardening, eliminando a necessidade de scripts avulsos.
    - **Configura√ß√£o:**
        - Timezone definido para `America/Sao_Paulo` em todos os n√≥s.
        - **Alpine:** Instala√ß√£o autom√°tica do pacote `tzdata` e link manual do `/etc/localtime`.
        - **Debian:** Configura√ß√£o via m√≥dulo nativo `timezone`.
    - **Resultado:** Logs e Backups agora possuem timestamps consistentes (-03 BRT).
## 2026-01-08
**Status:** ‚úÖ Sucesso (Infrastructure as Code)

**Foco:** Consolida√ß√£o do DockerHost e Migra√ß√£o para GitOps

- **Centraliza√ß√£o de Configura√ß√£o:**
    - Realizada a importa√ß√£o ("Adoption") de todas as configura√ß√µes manuais do DockerHost para o reposit√≥rio Git.
    - Estrutura padronizada em `configuration/dockerhost/{servi√ßo}`.
- **Automa√ß√£o (Ansible):**
    - Criado playbook `manage_stacks.yml` que atua como "Fonte da Verdade".
    - O playbook gerencia a sincroniza√ß√£o de arquivos, permiss√µes e execu√ß√£o dos containers.
- **Gest√£o de Segredos:**
    - Implementada l√≥gica h√≠brida no Ansible:
        - Servi√ßos simples (Traefik, Whoami) iniciados via m√≥dulo Docker direto.
        - Servi√ßos cr√≠ticos (Authentik, Vaultwarden) gerenciados via Systemd Units (`authentik-vault.service`) para garantir a inje√ß√£o de segredos do Vault via script `start-with-vault.sh`.
- **Resultado:**
    - O servidor DockerHost agora √© gerenciado remotamente. Altera√ß√µes s√£o feitas no Git e aplicadas via Ansible, garantindo consist√™ncia e eliminando "Snowflake Servers".
## 2026-01-07
**Status:** ‚úÖ Sucesso (Automa√ß√£o & Management Plane)

**Foco:** Cria√ß√£o da Torre de Controle (Ansible) e Saneamento de Rede

- **Infraestrutura de Rede (VLAN 10 - MGMT):**
    - Criada VLAN 10 no OPNsense (`10.10.10.1/24`) atribu√≠da √† interface `vtnet1` (Trunk), agrupando-a com as redes TRUSTED/SERVER.
    - **Decis√£o Arquitetural:** Mantida a separa√ß√£o f√≠sica/l√≥gica onde a VLAN 40 (Vault) reside na `vtnet0` (LAN Dedicada) e as demais na `vtnet1` (Trunk), respeitando o isolamento de seguran√ßa.
    - **Troubleshooting (Bloqueio L2):** O container na VLAN 10 n√£o conseguia comunicar com o Gateway.
        - *Causa:* A interface de rede da VM OPNsense no Proxmox (`net1`) possu√≠a um filtro de VLANs (`trunks=20;30;50`) que bloqueava a tag 10.
        - *Corre√ß√£o:* Editado `/etc/pve/qemu-server/100.conf` para incluir a VLAN 10 na lista de permitidos.
- **Management Node (LXC 102):**
    - Criado Container Alpine Linux (102 - Management) na VLAN 10.
    - **Configura√ß√£o:** IP Est√°tico `10.10.10.10`, acesso SSH via chave.
    - **Tooling:** Instalado Ansible (Core 2.17+), Restic, Terraform e Git.
- **Automa√ß√£o (Ansible):**
    - **Bootstrap:** Reposit√≥rio `homelab` clonado em `/opt/homelab`.
    - **Conectividade:** Chave SSH do Container autorizada no DockerHost (`10.10.30.10`).
    - **Corre√ß√£o no DockerHost:** O Debian Minimal n√£o possu√≠a `sudo`. Instalado pacote manualmente e configurado `NOPASSWD` para o usu√°rio de automa√ß√£o, destravando a execu√ß√£o de playbooks com `become: yes`.
    - **Primeiro Run:** Executado playbook `hardening_debian.yml` com sucesso.
        - *A√ß√£o:* Atualiza√ß√£o do OS, instala√ß√£o de ferramentas (fail2ban, htop, ncdu) e remo√ß√£o intencional do UFW (para evitar conflito com Docker/Traefik).
## 2026-01-05
**Status:** ‚úÖ Sucesso (Disaster Recovery & Validation) e adi√ß√£o do primeiro servi√ßo (Vaultwarden)

**Foco:** Teste de Resili√™ncia e Recupera√ß√£o de Falha Humana

- **O Incidente (Human Error):**
    - Durante troubleshooting de acesso, executei `docker compose down -v` no stack do Authentik.
    - **Impacto:** O flag `-v` deletou o volume persistente do PostgreSQL. O banco de dados de identidade foi zerado.
    - **Sintomas:** Perda de usu√°rios, grupos, policies e configura√ß√µes de Providers. O Vault e Traefik permaneceram intactos, mas o "porteiro" (Authentik) perdeu a mem√≥ria.
- **A Recupera√ß√£o (Cold Recovery):**
    - Recriado usu√°rio admin (`akadmin`).
    - Recriados os Providers e Applications para Traefik e Vault.
    - Restaurada a Policy Python (`infra-admins`) para RBAC.
    - **Tempo de Recupera√ß√£o:** ~15 minutos.
- **Teste de Fogo (Reboot do Host):**
    - Executado reboot total do servidor f√≠sico para validar a automa√ß√£o criada ontem.
    - **Comportamento Observado:**
        1.  Proxmox subiu e pediu senha LUKS (OK), desbloqueio realizado via SSH do Dropbear.
        2.  VMs iniciaram na ordem correta (OPNsense -> DNS -> Vault -> DockerHost).
        3.  **Resili√™ncia:** O servi√ßo `authentik-vault` no DockerHost falhou ao tentar conectar no Vault (que estava Selado). O Systemd entrou em loop de retry (OK).
        4.  **Interven√ß√£o:** Realizado Unseal manual do Vault via SSH.
        5.  **Sucesso:** Imediatamente ap√≥s o Unseal, o script do DockerHost obteve a senha do banco e subiu o Authentik automaticamente.
- **Conclus√£o:** A arquitetura de *AppRole* com inje√ß√£o de segredos em RAM provou-se resiliente a reboots e segura. O incidente refor√ßou a necessidade de **n√£o usar** `-v` em produ√ß√£o e a urg√™ncia de configurar backups automatizados do banco PostgreSQL.
- **Deploy de Aplica√ß√£o (Vaultwarden):**
    - **Objetivo:** Hospedar gerenciador de senhas soberano para validar a arquitetura de segredos (AppRole) e substituir depend√™ncia de nuvem.
    - **Decis√µes T√©cnicas:**
        - **Database:** Escolhido **SQLite** para reduzir complexidade e facilitar backup (arquivo √∫nico), em vez de adicionar overhead com PostgreSQL.
        - **Ingress:** Configura√ß√£o h√≠brida no Traefik:
            1.  `vaultwarden.home/` (API/Web): Acesso p√∫blico (interno) para compatibilidade com Apps Mobile.
            2.  `vaultwarden.home/admin`: Protegido por Middleware Authentik (`infra-admins` only).
    - **Automa√ß√£o:**
        - Criado script `start-with-vault.sh` espec√≠fico.
        - O DockerHost autentica no Vault via AppRole, busca o `ADMIN_TOKEN` e injeta no container.
        - **Valida√ß√£o de Seguran√ßa:** O token n√£o existe em texto plano no disco (apenas o SecretID com permiss√£o 600).
    - **Testes:**
        - **Web/Browser Extension:** Sucesso total. Login, sincroniza√ß√£o e acesso ao Admin (via Authentik) funcionando.
        - **Mobile (Android):** O App Bitwarden recusou conex√£o devido ao certificado autoassinado (SSL Handshake Error).
            - *Workaround:* Validado via extens√£o. A corre√ß√£o definitiva vir√° com a implementa√ß√£o de CA confi√°vel no Android ou Let's Encrypt.
    - **Backup:** Procedimento de backup semanal (JSON Criptografado) mantido.
## 2026-01-04
**Status:** ‚úÖ Sucesso (Refatora√ß√£o de Seguran√ßa)

**Foco:** Migra√ß√£o do Vault para VM Dedicada (Zero Trust Real)

- **Corre√ß√£o de Rumo:**
    - A implementa√ß√£o inicial (Container) violava a pol√≠tica de segmenta√ß√£o da VLAN 40.
    - **A√ß√£o:** Destru√≠ o container e provisionei a VM 106 (`Vault`) isolada na VLAN 40.
- **Infraestrutura:**
    - **OPNsense:** Criada VLAN 40 e regra de firewall permitindo apenas `Source: DockerHost` -> `Dest: Vault:8200`.
    - **Traefik:** Configurado *File Provider* para rotear `vault.home` para `http://10.10.40.10:8200` via arquivo din√¢mico.
- **Vault Setup:**
    - Instala√ß√£o nativa (apt) no Debian 13.
    - Configurado `api_addr = "https://vault.home"` para garantir que redirecionamentos de UI passem pelo Proxy reverso.
    - **Resultado:** Unseal realizado com sucesso, chaves salvas e interface protegida pelo Authentik.
- **Hardening Final & Valida√ß√£o (P√≥s-Migra√ß√£o):**
    - **Host Firewall (Defense in Depth):** Ativado UFW na VM Vault para n√£o depender apenas do OPNsense.
        - Regras aplicadas: `Allow 8200 from 10.10.30.10` e `Allow 22 from Trusted/Mgmt`.
        - Teste de movimento lateral (SSH do DockerHost para Vault): **Bloqueado com sucesso**.
    - **Isolamento de Internet:** Regra `Temp Install Vault` desativada no OPNsense.
        - Teste: `ping 1.1.1.1` a partir do Vault falha (Timeout). A VM est√° isolada.
    - **Troubleshooting de Rede:**
        - Resolvido problema onde a VM n√£o conectava √† internet para updates iniciais.
        - **Causa:** Falta de regra de **Outbound NAT** para a nova VLAN 40. Corrigido adicionando regra manual no OPNsense.
- **Integra√ß√£o Zero Trust (Vault + Authentik):**
    - **Desafio:** O DockerHost precisava ler segredos sem interven√ß√£o humana, mas o Vault inicia trancado (Sealed) ap√≥s reboot.
    - **Solu√ß√£o:**
        1.  **Identidade:** Configurei **AppRole** no Vault. O DockerHost possui um "crach√°" (SecretID) protegido em `/etc/vault/dockerhost.secretid` (root-only).
        2.  **Rede:** Ajustei o DNS do DockerHost para usar o AdGuard (`10.10.30.5`) via `systemd-resolved`, garantindo resolu√ß√£o de `vault.home` sem hacks manuais.
        3.  **Automa√ß√£o:** Desenvolvi o script `start-with-vault.sh` que autentica, baixa a senha do PostgreSQL e sobe o stack.
    - **Teste de Resili√™ncia:**
        - Realizado rein√≠cio f√≠sico do servidor (Cold Boot).
        - O Vault subiu selado. O servi√ßo `authentik-vault` entrou em loop de retry no DockerHost (comprovando resili√™ncia).
        - Ap√≥s destrancar o Vault manualmente via SSH (lembrando de definir `export VAULT_ADDR='http://127.0.0.1:8200'`), o DockerHost detectou o sucesso automaticamente e subiu os containers do Authentik em menos de 10 segundos.
    - **Resultado:** Infraestrutura resiliente a falhas de energia e sem segredos em texto puro no disco.
## 2026-01-03
**Status:** ‚úÖ Sucesso (Secret Management)

**Foco:** Implementa√ß√£o do HashiCorp Vault

- **Decis√£o de Vers√£o:**
    - Optado por **Vault v1.21.1** (Latest Stable), garantindo corre√ß√µes de seguran√ßa recentes.
- **Implementa√ß√£o:**
    - Backend de armazenamento: **Raft** (Integrated Storage) - elimina depend√™ncia do Consul.
    - Prote√ß√£o de Ingress: Middleware `authentik@docker` aplicado no router do Vault. Apenas admins autenticados chegam na tela de login do cofre.
- **Cerim√¥nia de Inicializa√ß√£o (Unseal):**
    - Executada inicializa√ß√£o com **Shamir's Secret Sharing**.
    - **Configura√ß√£o:** 5 Key Shares, Threshold de 3 chaves para desbloqueio.
    - **Root Token:** Gerado e armazenado com seguran√ßa m√°xima (Bitwarden, depois para o Vaultwarden) junto com as 5 chaves de unseal.
- **Estado Final:**
    - Vault operacional em `https://vault.home`.
    - Banco de dados criptografado em repouso.
    - Requer desbloqueio manual (3 chaves) a cada reinicializa√ß√£o do container.
## 2026-01-02 (Parte 4)
**Status:** ‚úÖ Sucesso (Hardening RBAC)

**Foco:** Restri√ß√£o de Acesso via Policy (Python)

- **Objetivo:** Impedir que qualquer usu√°rio logado no Authentik (mesmo sem privil√©gios) acesse o painel administrativo do Traefik. Apenas a equipe de infraestrutura deve ter acesso.
- **Implementa√ß√£o:**
    - Criado grupo `infra-admins` no Authentik e inclu√≠do o usu√°rio administrador.
    - Criada uma **Expression Policy** (Python) para validar a pertin√™ncia ao grupo:
      ```python
      return ak_is_group_member(request.user, name="infra-admins")
      ```
    - Vinculada a policy ao aplicativo `Traefik Dashboard` com prioridade 0.
- **Valida√ß√£o:**
    - Login com admin: **Sucesso** (Acesso liberado).
    - Login com usu√°rio comum: **Bloqueado** (Mensagem "Permission Denied" exibida pelo Authentik).
## 2026-01-02 (Parte 3)
**Status:** ‚úÖ Sucesso (Identity Provider & Zero Trust)

**Foco:** Implementa√ß√£o do Authentik e Integra√ß√£o com Traefik (ForwardAuth)

- **Desafio 1 (Erro Operacional):**
    - Durante a configura√ß√£o dos arquivos `docker-compose.yml`, houve uma **sobrescrita acidental** do arquivo do Authentik com o conte√∫do do Traefik. Isso causou a queda de ambos os servi√ßos.
    - *Recupera√ß√£o:* Foi necess√°rio restaurar manualmente os manifestos YAML corretos em `/opt/auth/authentik` e `/opt/traefik` e recriar os containers (`force-recreate`).
- **Desafio 2 (O Erro 404 no Callback):**
    - Ap√≥s configurar o middleware, o fluxo de login iniciava, mas falhava no retorno (`/outpost.goauthentik.io/callback...`) com erro 404 do Traefik.
    - **Causa T√©cnica:** O Traefik bloqueava a URL de callback porque ela n√£o correspondia √† regra restrita do Dashboard (`PathPrefix(/dashboard)`).
- **Solu√ß√£o Definitiva (Global Callback Route):**
    - Adicionada uma Label no servi√ßo do Authentik criando um Router dedicado: `Rule=PathPrefix(/outpost.goauthentik.io/)`.
    - Isso instrui o Traefik a interceptar *qualquer* requisi√ß√£o de callback do Authentik, independente do dom√≠nio, e encaminh√°-la para o container do IdP.
- **Resultado:**
    - Acesso a `https://traefik.home/dashboard/` redireciona para `auth.home`, exige credenciais e retorna com sucesso.
    - Porta 8080 do Traefik foi fechada definitivamente.
## 2026-01-02 (Parte 2)
**Status:** ‚úÖ Sucesso (Hardening)

**Foco:** Seguran√ßa do DockerHost e Padroniza√ß√£o

- **Motiva√ß√£o:** Antes de implementar a camada de identidade (Authentik), identifiquei que o Traefik mantinha acesso direto e irrestrito ao `docker.sock`. Isso violava o princ√≠pio do menor privil√©gio (Security by Design).
- **A√ß√µes de Mitiga√ß√£o:**
    - **Socket Proxy:** Interpus um proxy que filtra chamadas de API. Agora o Traefik s√≥ tem permiss√£o para listar containers (`GET`). Comandos destrutivos ou de cria√ß√£o (`POST`, `DELETE`) s√£o bloqueados silenciosamente.
    - **Resili√™ncia de Disco:** Configurei rota√ß√£o de logs global no Docker Daemon (Max 3 arquivos de 10MB) para evitar que servi√ßos verbosos lotem o armazenamento de 32GB.
    - **OS Patching:** Debian configurado para aplicar patches de seguran√ßa automaticamente (`unattended-upgrades`).
    - **Organiza√ß√£o:** Migrei servi√ßos dispersos para a hierarquia `/opt/services/` e padronizei o ownership para o usu√°rio comum, removendo a necessidade de operar arquivos como root.
## 2026-01-02
**Status:** ‚úÖ Sucesso Definitivo (Traefik v3.6)

**Foco:** Upgrade para Traefik v3.6 (Latest Stable) e Valida√ß√£o de Ingress

- **Decis√£o Estrat√©gica:**
    - Optado por n√£o manter a vers√£o legado (v2.11) e migrar imediatamente para **Traefik v3.6** para evitar d√≠vida t√©cnica futura (EOL em Fev/2026).
- **Implementa√ß√£o (The Fix):**
    - Configurado container `traefik:v3.6`.
    - Mantida a vari√°vel de ambiente `DOCKER_API_VERSION=1.45`.
    - **Resultado:** A biblioteca client do Traefik v3 respeitou a vari√°vel e ignorou a negocia√ß√£o de vers√£o falha, conectando-se perfeitamente ao Docker Engine do Debian 13.
- **Valida√ß√£o T√©cnica (Headers):**
    - `whoami` reportou `X-Forwarded-Proto: https` (Termina√ß√£o SSL OK).
    - `X-Real-Ip: 10.10.20.101` (Roteamento de VLANs transparente, sem mascaramento de IP).
    - Logs do Traefik limpos, sem erros de API.
## 2025-12-31
**Status:** ‚úÖ Sucesso (Traefik & Ingress)

**Foco:** Implementa√ß√£o do Proxy Reverso (Traefik) e Compatibilidade Docker API

- **Desafio (Dependency Hell):**
    - O Docker Engine no **Debian 13 (Trixie)** exige API m√≠nima `1.44`.
    - O **Traefik v3** tenta negociar vers√µes antigas (`1.24`) por padr√£o e falha em ambientes *bleeding edge*.
    - Tentativas de for√ßar a vers√£o via flags (`--providers.docker.apiVersion`) ou vari√°veis (`DOCKER_API_VERSION`) no Traefik v3 falharam silenciosamente devido a mudan√ßas recentes na lib interna.
- **Solu√ß√£o (Downgrade T√°tico):**
    - Revertido para **Traefik v2.11** (LTS).
    - Injetada vari√°vel de ambiente `DOCKER_API_VERSION=1.45` diretamente no container.
    - Isso for√ßou o cliente Docker interno do Traefik a falar a l√≠ngua do Debian 13 sem negocia√ß√£o.
- **Valida√ß√£o:**
    - Acesso a `https://whoami.home` confirmado.
    - Redirecionamento HTTP -> HTTPS (80 -> 443) ativo.
    - **Header X-Real-IP:** O container recebe o IP real do cliente (`10.10.20.x`), confirmando que o roteamento Inter-VLAN est√° transparente.
- **Observa√ß√£o:**
    - Atualizar assim que poss√≠vel para a vers√£o mais recente (v2.11 ends Feb 01, 2026).
## 2025-12-30
**Status:** ‚úÖ Sucesso (DNS & Privacy)

**Foco:** Implementa√ß√£o do AdGuard Home e Gest√£o de DNS
- **Infraestrutura DNS (LXC Container):**
    - Criado Container LXC `101 (AdGuard-Primary)` baseado em Alpine Linux (3.23) na VLAN 30.
    - **Specs:** 1 Core, 256MB RAM, IP Est√°tico `10.10.30.5`.
    - **Software:** AdGuard Home instalado via script oficial.
        - `curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh | sh -s -- -v`, dispon√≠vel [aqui](https://github.com/AdguardTeam/AdGuardHome
).
- **Configura√ß√£o do Servi√ßo (AdGuard):**
    - **Upstreams:** Configurados servidores DNS-over-HTTPS (Cloudflare/Quad9) para garantir privacidade e evitar intercepta√ß√£o de porta 53 pelo ISP.
    - **Reverse DNS:** Apontado para o OPNsense (`10.10.30.1`) para resolu√ß√£o correta de hostnames locais nos logs.
- **Integra√ß√£o de Rede (OPNsense DHCP):**
    - Alterado o servidor DNS entregue via DHCP para as VLANs **TRUSTED (20)** e **IOT (50)**:
        - **De:** `1.1.1.1` e `8.8.8.8` (Externos (Cloudflare e Google, respectivamente).
        - **Para:** `10.10.30.5` (Local (AdGuard)).
    - **Pol√≠tica de Resili√™ncia:** A VLAN **SERVER (30)** teve seu DNS mantido em `1.1.1.1` para evitar depend√™ncia c√≠clica (o DockerHost n√£o deve depender de um container vizinho para resolver nomes durante o boot).
- **Valida√ß√£o:**
    - Cliente Arch Linux (VLAN 20) renovou DHCP e confirmou recebimento do DNS `10.10.30.5` via `/etc/resolv.conf`.
    - Dashboard do AdGuard registrou queries vindas da rede TRUSTED e bloqueios ativos.
    - O mesmo foi realizado com a VLAN 50.
- Documenta√ß√£o do repo melhor documentada e formatada.
- Repo aberto.
## 2025-12-29
**Status:** ‚úÖ Sucesso (Docker & Hardening)

**Foco:** Configura√ß√£o do DockerHost e Ajuste de Firewall

- **Hardening SSH:**
    - Chaves Ed25519 copiadas do Arch Linux para o DockerHost.
    - **Configura√ß√£o de Seguran√ßa:** Editado `/etc/ssh/sshd_config` para:
        * `PermitRootLogin no` (Bloqueio total de login direto como root via SSH).
        * `PasswordAuthentication no` (Autentica√ß√£o por senha desativada; apenas chaves SSH).
        * `PubkeyAuthentication yes` (Autentica√ß√£o por chave p√∫blica habilitada).
        * `ChallengeResponseAuthentication no` (Desativa m√©todos interativos/legados de autentica√ß√£o).
        * `UsePAM yes` (Mant√©m PAM ativo para controle de sess√£o e pol√≠ticas do sistema).
    - **Valida√ß√£o:** Login verificado com sucesso via chave; tentativa de login por senha rejeitada como esperado.

- **Instala√ß√£o do Docker:**
    - Utilizado reposit√≥rio oficial (m√©todo compat√≠vel com Debian Trixie/Bookworm).
    - Engine e Plugin Compose (v5.0.0) instalados.
    - Usu√°rio adicionado ao grupo `docker` para execu√ß√£o sem root/sudo.
    - **Teste de Sanidade:** `docker run hello-world` executado com sucesso (Pull da imagem via WAN OK, Execu√ß√£o OK).

- **Incidente de Conectividade (Firewall):**
    - *Sintoma:* O Arch Linux (VLAN 20) n√£o conseguia pingar ou conectar via SSH no DockerHost (VLAN 30), resultando em Timeout.
    - *Causa Raiz:* Esquecimento da pol√≠tica de "Default Deny". Embora a VLAN 30 tivesse permiss√£o de sa√≠da (para internet), a VLAN 20 n√£o tinha permiss√£o expl√≠cita de **entrada/passagem** para a VLAN 30.
    - *Solu√ß√£o:* Criada regra de Firewall na interface **TRUSTED**:
        - **Action:** Pass
        - **Source:** TRUSTED net
        - **Destination:** Any (ou SERVER net)
        - **Justificativa:** Permite que dispositivos de gerenciamento acessem os servidores.
        - O mesmo foi feito com a VLAN 50 (IOT).
## 2025-12-28
**Status:** ‚ö†Ô∏è Resgate de Rede (Driver Migration)

**Foco:** Recupera√ß√£o das VLANs ap√≥s mudan√ßa para VirtIO

- **O Incidente:**
    - Ao verificar a VM `DockerHost`, notei que ela n√£o pegava IP (estava com APIPA `169.254.x.x`).
    - No OPNsense, as interfaces **TRUSTED**, **SERVER** e **IOT** haviam desaparecido do painel de controle, restando apenas LAN e WAN.
- **Diagn√≥stico:**
    - A mudan√ßa do driver de rede da VM OPNsense (de `e1000` para `VirtIO`) alterou a nomenclatura das interfaces no BSD (de `em0` para `vtnet0/1`).
    - Isso quebrou a associa√ß√£o "Parent Interface" das VLANs, tornando-as √≥rf√£s e desativadas.
    - Identifiquei via MAC Address (`04:FD`) que a interface `vtnet1` (atualmente WAN) era, na verdade, a porta f√≠sica configurada com Trunks no Proxmox.
- **Solu√ß√£o:**
    1. **Reparenting:** Reconfigurei as VLANs 20, 30 e 50 para usarem a interface correta (`vtnet1`) como pai.
    2. **Re-assignment:** Re-adicionei as interfaces l√≥gicas que haviam sumido.
    3. **Re-IP:** Restaurei os IPs Est√°ticos (`10.10.x.1`) e servi√ßos DHCP que foram limpos durante a falha.
- **Resultado:** A VM DockerHost obteve o IP `10.10.30.102` imediatamente ap√≥s o fix.
## 2025-12-27
**Status:** ‚úÖ Sucesso

**Foco:** Provisionamento do DockerHost e Segmenta√ß√£o VLAN 30

- **Infraestrutura de Rede (VLAN 30 - SERVER):**
    - Configurada interface l√≥gica no OPNsense (`10.10.30.1/24`) com DHCP ativado (`.100` a `.200`).
    - Validado isolamento: `ping` da VLAN 20 (Trusted) para 50 (IoT) falha como esperado (Bloqueio padr√£o).
    - Regras de Firewall: Criada regra tempor√°ria "Pass All" na VLAN 30 para permitir instala√ß√£o de pacotes.
- **Computa√ß√£o (VM DockerHost):**
    - Criada VM ID `105` (Debian 13 Minimal (somente com SSH Server e Standard system utilities)).
    - **Specs:** 2 vCores (Host), 8GB RAM (Static), 32GB Disk (VirtIO Block).
    - **Rede:** Interface VirtIO com **Tag 30** definida no Proxmox.
    - **Valida√ß√£o:**
        - VM obteve IP `10.10.30.x` automaticamente.
        - Conectividade externa (WAN) funcionando via NAT H√≠brido.
        - Acesso SSH verificado a partir da VLAN 20 (Trusted).
## 2025-12-26
**Status:** ‚úÖ Sucesso Cr√≠tico (Rede Funcional)

**Foco:** Troubleshooting de VLANs, Switch e Roteamento OPNsense

- **O Incidente:** O DHCP n√£o chegava aos clientes via Wi-Fi (VLANs 20/50) e, quando chegava (ap√≥s fix), n√£o havia navega√ß√£o.
- **Diagn√≥stico e Solu√ß√µes (Post-Mortem):**
    1. **Proxmox Bridge Dropping Tags:** A bridge `vmbr0` (VLAN Aware) estava descartando pacotes taggeados (20, 50) antes de entreg√°-los √† VM.
        - *Corre√ß√£o:* Adicionado `bridge-vids 2-4094` em `/etc/network/interfaces` no Host.
        - *Corre√ß√£o:* Adicionado `trunks=20;50` na configura√ß√£o da interface de rede da VM (`/etc/pve/qemu-server/100.conf`).
    2. **Conflito de Roteamento (Routing Loop):** A interface LAN (`192.168.0.250/24`) e WAN (`192.168.0.50/24`) estavam na mesma sub-rede. O kernel do OPNsense entrava em conflito de rota ao tentar responder a pacotes de outras VLANs, causando erro *"Provide a valid source address"* no Ping.
        - *Solu√ß√£o Definitiva:* Alterado IP da LAN para `192.168.99.1/24` para isolar as redes.
    3. **Hardware Offloading (VirtIO):** Pacotes DHCP chegavam corrompidos/descartados.
        - *Ajuste:* Desativado Hardware CRC, TSO e LRO nas configura√ß√µes do OPNsense.
    4. **Firewall Block:** VLANs novas v√™m com "Default Deny".
        - *Ajuste:* Criadas regras de "Pass All" e configurado Outbound NAT H√≠brido.
## 2025-12-25
**Status:** üîÑ Troca de Hardware

**Foco:** Aquisi√ß√£o de Storage para Bitcoin Node

- **Problema Log√≠stico:** O SSD SanDisk (comprado em 14/12) entrou em estado de atraso indefinido no Mercado Livre ("Em prepara√ß√£o" por 10 dias). Compra cancelada para evitar parada no projeto.
- **Revis√£o T√©cnica:** Aproveitei o incidente para reavaliar a especifica√ß√£o. Identifiquei que o SanDisk Plus √© **DRAM-less**. Para um Full Node Bitcoin, isso seria catastr√≥fico durante o IBD (Initial Block Download), pois o esgotamento do cache SLC derrubaria a velocidade de escrita drasticamente.
- **Decis√£o:** Adquirido **Samsung 870 EVO 2TB** (Envio Full).
    - Embora o custo seja marginalmente maior, ele possui **2GB de Cache LPDDR4** e controlador MKX. Isso garante que a sincroniza√ß√£o da blockchain ocorra na velocidade m√°xima da interface SATA, economizando dias de espera futura.
    - A placa de rede HP NC364T (incompat√≠vel) devolvida tamb√©m serviu para abater a diferen√ßa de custo.
## 2025-12-24
**Status:** ‚ö†Ô∏è Resgate de Rede (Rollback)

**Foco:** Recupera√ß√£o de Acesso e Simplifica√ß√£o de Rede

- **O Incidente:**
    - Ap√≥s o sucesso inicial com o Dropbear, tentamos migrar para a topologia "Router-on-a-Stick" configurando VLANs (10, 20, 90) no OPNsense e no Switch.
    - **Resultado:** Perda total de acesso (Lockout). O Dropbear parou de responder e o Proxmox ficou inacess√≠vel.
- **Diagn√≥stico (A Causa Raiz):**
    1. **Hardcoding no Boot:** O arquivo `/etc/initramfs-tools/initramfs.conf` continha uma linha for√ßando IP Est√°tico (`IP:10.10.10.1...`).
    2. **Desalinhamento:** O Switch foi configurado para esperar VLANs, mas o servidor bootava for√ßando um IP fora da sub-rede e sem tagging, causando falha de comunica√ß√£o.
- **A Solu√ß√£o (O Resgate):**
    - **Physical Reset:** Reset f√≠sico do Switch TP-Link para configura√ß√µes de f√°brica (Rede Flat 192.168.0.x).
    - **Boot Config:** Editado `initramfs.conf` para remover o IP est√°tico e definir `IP=dhcp`.
    - **Proxmox Config:** Editado `/etc/network/interfaces` para usar DHCP na `vmbr0`.
- **Li√ß√£o Aprendida:**
    - **NUNCA** definir IPs est√°ticos no `initramfs` em ambiente de Homelab. Usar `IP=dhcp` e controlar a fixa√ß√£o de IP via reserva no Roteador (DHCP Static Lease).
    - O Dropbear (Desbloqueio) deve permanecer sempre na VLAN Nativa/Untagged (Rede "Burra") para garantir acesso de emerg√™ncia independente do estado do OPNsense.
## 2025-12-22
**Status:** ‚úÖ Sucesso Total

**Foco:** Otimiza√ß√£o de Hardware e Router-on-a-Stick

- **Decis√£o T√©cnica:** A placa HP Quad-Port foi removida. O custo de complexidade de driver e energia n√£o justificava o uso, dado que o switch TP-Link gerencia VLANs com perfei√ß√£o.
- **Troubleshooting Dropbear:** Ap√≥s a remo√ß√£o da placa HP, o nome da interface mudou de `enp8s0` para `enp4s0`. Isso quebrou o desbloqueio remoto inicial.
    - *Corre√ß√£o:* Atualizei o `initramfs.conf` com `DEVICE=enp4s0` e fixei a porta `2222`. O teste de `cryptroot-unlock` via SSH no notebook Arch funcionou ap√≥s limpar o `known_hosts`.
- **OPNsense:** WAN configurada com sucesso na VLAN 90. O IP foi obtido via DHCP do modem em modo DMZ.
## 2025-12-21
**Status:** ‚úÖ Sucesso

**Foco:** Criptografia (FDE), Swap e Desbloqueio Remoto

- **LUKS:** Realizei a convers√£o p√≥s-instala√ß√£o do Proxmox para **LUKS2** (Full Disk Encryption) seguindo o guia manual. 
- **Swap:** Configurei um **ZFS Swap de 16GB** para evitar travamentos por exaust√£o de mem√≥ria (OOM), j√° que o ZFS sem swap pode entrar em deadlock.
- **Dropbear:** Configurei o servidor SSH leve (Dropbear) no initramfs.
    - **Teste:** Reiniciei o servidor sem monitor. Conectei via SSH na porta tempor√°ria, digitei a senha do disco e o boot do Proxmox prosseguiu corretamente.

## 2025-12-20
**Status:** ‚úÖ Sucesso

**Foco:** Dry Run (Instala√ß√£o e Rede)

- **Instala√ß√£o Base:** Instalei o Proxmox VE 9.1 para validar a detec√ß√£o de hardware.
- **Rede:**
    - A interface Onboard foi identificada como `eno1` (Driver `r8169`).
    - A placa HP Quad-Port foi identificada corretamente (Driver `e1000e`).
    - **Lat√™ncia:** Teste de ping direto registrou `0.2ms`.
- **Armazenamento:** O **ZFS Mirror (RAID 1)** foi montado e ativado no `rpool` com os dois NVMe Kingston.
- **Troubleshooting:** Tive dificuldade inicial para pingar o servidor (10.10.10.x) a partir do meu Arch Linux.
    - *Solu√ß√£o:* Era necess√°rio ajustar as regras de entrada/sa√≠da no firewall do cliente (Arch), pois n√£o h√° roteador intermediando a conex√£o f√≠sica direta neste est√°gio.

## 2025-12-19
**Status:** ‚úÖ Sucesso

**Foco:** Hardware Burn-in e BIOS

- **Valida√ß√£o de Mem√≥ria:** Executei o **MemTest86 V11.5** por 6 horas e 17 minutos.
    - **Resultado:** 48/48 testes completados com **0 Erros**.
    - *Telemetria:* XMP validado a 3192 MT/s. A temperatura m√°xima da CPU ficou em 48¬∞C, validando a instala√ß√£o do cooler AK400.
![Evid√™ncia do MemTest86](https://github.com/fajremvp/homelab/blob/main/docs/assets/benchmarks/MemTest86.jpeg)
- **Configura√ß√£o da BIOS:** Apliquei as configura√ß√µes cr√≠ticas na Gigabyte B760M.
