# Di√°rio de Bordo

Este arquivo documenta a jornada, erros, aprendizados e decis√µes di√°rias.
Para mudan√ßas estruturais formais, veja o [CHANGELOG](../CHANGELOG.md).

---
## 2026-01-08
**Status:** ‚úÖ Sucesso (Infrastructure as Code)

**Foco:** Consolida√ß√£o do DockerHost e Migra√ß√£o para GitOps

- **Centraliza√ß√£o de Configura√ß√£o:**
    - Realizada a importa√ß√£o ("Adoption") de todas as configura√ß√µes manuais do DockerHost para o reposit√≥rio Git.
    - Estrutura padronizada em `configuration/dockerhost/{servi√ßo}`.
- **Automa√ß√£o (Ansible):**
    - Criado playbook `manage_stacks.yml` que atua como "Fonte da Verdade".
    - O playbook gerencia a sincroniza√ß√£o de arquivos, permiss√µes e execu√ß√£o dos containers.
- **Gest√£o de Segredos:**
    - Implementada l√≥gica h√≠brida no Ansible:
        - Servi√ßos simples (Traefik, Whoami) iniciados via m√≥dulo Docker direto.
        - Servi√ßos cr√≠ticos (Authentik, Vaultwarden) gerenciados via Systemd Units (`authentik-vault.service`) para garantir a inje√ß√£o de segredos do Vault via script `start-with-vault.sh`.
- **Resultado:**
    - O servidor DockerHost agora √© gerenciado remotamente. Altera√ß√µes s√£o feitas no Git e aplicadas via Ansible, garantindo consist√™ncia e eliminando "Snowflake Servers".
## 2026-01-07
**Status:** ‚úÖ Sucesso (Automa√ß√£o & Management Plane)

**Foco:** Cria√ß√£o da Torre de Controle (Ansible) e Saneamento de Rede

- **Infraestrutura de Rede (VLAN 10 - MGMT):**
    - Criada VLAN 10 no OPNsense (`10.10.10.1/24`) atribu√≠da √† interface `vtnet1` (Trunk), agrupando-a com as redes TRUSTED/SERVER.
    - **Decis√£o Arquitetural:** Mantida a separa√ß√£o f√≠sica/l√≥gica onde a VLAN 40 (Vault) reside na `vtnet0` (LAN Dedicada) e as demais na `vtnet1` (Trunk), respeitando o isolamento de seguran√ßa.
    - **Troubleshooting (Bloqueio L2):** O container na VLAN 10 n√£o conseguia comunicar com o Gateway.
        - *Causa:* A interface de rede da VM OPNsense no Proxmox (`net1`) possu√≠a um filtro de VLANs (`trunks=20;30;50`) que bloqueava a tag 10.
        - *Corre√ß√£o:* Editado `/etc/pve/qemu-server/100.conf` para incluir a VLAN 10 na lista de permitidos.
- **Management Node (LXC 102):**
    - Criado Container Alpine Linux (102 - Management) na VLAN 10.
    - **Configura√ß√£o:** IP Est√°tico `10.10.10.10`, acesso SSH via chave.
    - **Tooling:** Instalado Ansible (Core 2.17+), Restic, Terraform e Git.
- **Automa√ß√£o (Ansible):**
    - **Bootstrap:** Reposit√≥rio `homelab` clonado em `/opt/homelab`.
    - **Conectividade:** Chave SSH do Container autorizada no DockerHost (`10.10.30.10`).
    - **Corre√ß√£o no DockerHost:** O Debian Minimal n√£o possu√≠a `sudo`. Instalado pacote manualmente e configurado `NOPASSWD` para o usu√°rio de automa√ß√£o, destravando a execu√ß√£o de playbooks com `become: yes`.
    - **Primeiro Run:** Executado playbook `hardening_debian.yml` com sucesso.
        - *A√ß√£o:* Atualiza√ß√£o do OS, instala√ß√£o de ferramentas (fail2ban, htop, ncdu) e remo√ß√£o intencional do UFW (para evitar conflito com Docker/Traefik).
## 2026-01-05
**Status:** ‚úÖ Sucesso (Disaster Recovery & Validation) e adi√ß√£o do primeiro servi√ßo (Vaultwarden)

**Foco:** Teste de Resili√™ncia e Recupera√ß√£o de Falha Humana

- **O Incidente (Human Error):**
    - Durante troubleshooting de acesso, executei `docker compose down -v` no stack do Authentik.
    - **Impacto:** O flag `-v` deletou o volume persistente do PostgreSQL. O banco de dados de identidade foi zerado.
    - **Sintomas:** Perda de usu√°rios, grupos, policies e configura√ß√µes de Providers. O Vault e Traefik permaneceram intactos, mas o "porteiro" (Authentik) perdeu a mem√≥ria.
- **A Recupera√ß√£o (Cold Recovery):**
    - Recriado usu√°rio admin (`akadmin`).
    - Recriados os Providers e Applications para Traefik e Vault.
    - Restaurada a Policy Python (`infra-admins`) para RBAC.
    - **Tempo de Recupera√ß√£o:** ~15 minutos.
- **Teste de Fogo (Reboot do Host):**
    - Executado reboot total do servidor f√≠sico para validar a automa√ß√£o criada ontem.
    - **Comportamento Observado:**
        1.  Proxmox subiu e pediu senha LUKS (OK), desbloqueio realizado via SSH do Dropbear.
        2.  VMs iniciaram na ordem correta (OPNsense -> DNS -> Vault -> DockerHost).
        3.  **Resili√™ncia:** O servi√ßo `authentik-vault` no DockerHost falhou ao tentar conectar no Vault (que estava Selado). O Systemd entrou em loop de retry (OK).
        4.  **Interven√ß√£o:** Realizado Unseal manual do Vault via SSH.
        5.  **Sucesso:** Imediatamente ap√≥s o Unseal, o script do DockerHost obteve a senha do banco e subiu o Authentik automaticamente.
- **Conclus√£o:** A arquitetura de *AppRole* com inje√ß√£o de segredos em RAM provou-se resiliente a reboots e segura. O incidente refor√ßou a necessidade de **n√£o usar** `-v` em produ√ß√£o e a urg√™ncia de configurar backups automatizados do banco PostgreSQL.
- **Deploy de Aplica√ß√£o (Vaultwarden):**
    - **Objetivo:** Hospedar gerenciador de senhas soberano para validar a arquitetura de segredos (AppRole) e substituir depend√™ncia de nuvem.
    - **Decis√µes T√©cnicas:**
        - **Database:** Escolhido **SQLite** para reduzir complexidade e facilitar backup (arquivo √∫nico), em vez de adicionar overhead com PostgreSQL.
        - **Ingress:** Configura√ß√£o h√≠brida no Traefik:
            1.  `vaultwarden.home/` (API/Web): Acesso p√∫blico (interno) para compatibilidade com Apps Mobile.
            2.  `vaultwarden.home/admin`: Protegido por Middleware Authentik (`infra-admins` only).
    - **Automa√ß√£o:**
        - Criado script `start-with-vault.sh` espec√≠fico.
        - O DockerHost autentica no Vault via AppRole, busca o `ADMIN_TOKEN` e injeta no container.
        - **Valida√ß√£o de Seguran√ßa:** O token n√£o existe em texto plano no disco (apenas o SecretID com permiss√£o 600).
    - **Testes:**
        - **Web/Browser Extension:** Sucesso total. Login, sincroniza√ß√£o e acesso ao Admin (via Authentik) funcionando.
        - **Mobile (Android):** O App Bitwarden recusou conex√£o devido ao certificado autoassinado (SSL Handshake Error).
            - *Workaround:* Validado via extens√£o. A corre√ß√£o definitiva vir√° com a implementa√ß√£o de CA confi√°vel no Android ou Let's Encrypt.
    - **Backup:** Procedimento de backup semanal (JSON Criptografado) mantido.
## 2026-01-04
**Status:** ‚úÖ Sucesso (Refatora√ß√£o de Seguran√ßa)

**Foco:** Migra√ß√£o do Vault para VM Dedicada (Zero Trust Real)

- **Corre√ß√£o de Rumo:**
    - A implementa√ß√£o inicial (Container) violava a pol√≠tica de segmenta√ß√£o da VLAN 40.
    - **A√ß√£o:** Destru√≠ o container e provisionei a VM 106 (`Vault`) isolada na VLAN 40.
- **Infraestrutura:**
    - **OPNsense:** Criada VLAN 40 e regra de firewall permitindo apenas `Source: DockerHost` -> `Dest: Vault:8200`.
    - **Traefik:** Configurado *File Provider* para rotear `vault.home` para `http://10.10.40.10:8200` via arquivo din√¢mico.
- **Vault Setup:**
    - Instala√ß√£o nativa (apt) no Debian 13.
    - Configurado `api_addr = "https://vault.home"` para garantir que redirecionamentos de UI passem pelo Proxy reverso.
    - **Resultado:** Unseal realizado com sucesso, chaves salvas e interface protegida pelo Authentik.
- **Hardening Final & Valida√ß√£o (P√≥s-Migra√ß√£o):**
    - **Host Firewall (Defense in Depth):** Ativado UFW na VM Vault para n√£o depender apenas do OPNsense.
        - Regras aplicadas: `Allow 8200 from 10.10.30.10` e `Allow 22 from Trusted/Mgmt`.
        - Teste de movimento lateral (SSH do DockerHost para Vault): **Bloqueado com sucesso**.
    - **Isolamento de Internet:** Regra `Temp Install Vault` desativada no OPNsense.
        - Teste: `ping 1.1.1.1` a partir do Vault falha (Timeout). A VM est√° isolada.
    - **Troubleshooting de Rede:**
        - Resolvido problema onde a VM n√£o conectava √† internet para updates iniciais.
        - **Causa:** Falta de regra de **Outbound NAT** para a nova VLAN 40. Corrigido adicionando regra manual no OPNsense.
- **Integra√ß√£o Zero Trust (Vault + Authentik):**
    - **Desafio:** O DockerHost precisava ler segredos sem interven√ß√£o humana, mas o Vault inicia trancado (Sealed) ap√≥s reboot.
    - **Solu√ß√£o:**
        1.  **Identidade:** Configurei **AppRole** no Vault. O DockerHost possui um "crach√°" (SecretID) protegido em `/etc/vault/dockerhost.secretid` (root-only).
        2.  **Rede:** Ajustei o DNS do DockerHost para usar o AdGuard (`10.10.30.5`) via `systemd-resolved`, garantindo resolu√ß√£o de `vault.home` sem hacks manuais.
        3.  **Automa√ß√£o:** Desenvolvi o script `start-with-vault.sh` que autentica, baixa a senha do PostgreSQL e sobe o stack.
    - **Teste de Resili√™ncia:**
        - Realizado rein√≠cio f√≠sico do servidor (Cold Boot).
        - O Vault subiu selado. O servi√ßo `authentik-vault` entrou em loop de retry no DockerHost (comprovando resili√™ncia).
        - Ap√≥s destrancar o Vault manualmente via SSH (lembrando de definir `export VAULT_ADDR='http://127.0.0.1:8200'`), o DockerHost detectou o sucesso automaticamente e subiu os containers do Authentik em menos de 10 segundos.
    - **Resultado:** Infraestrutura resiliente a falhas de energia e sem segredos em texto puro no disco.
## 2026-01-03
**Status:** ‚úÖ Sucesso (Secret Management)

**Foco:** Implementa√ß√£o do HashiCorp Vault

- **Decis√£o de Vers√£o:**
    - Optado por **Vault v1.21.1** (Latest Stable), garantindo corre√ß√µes de seguran√ßa recentes.
- **Implementa√ß√£o:**
    - Backend de armazenamento: **Raft** (Integrated Storage) - elimina depend√™ncia do Consul.
    - Prote√ß√£o de Ingress: Middleware `authentik@docker` aplicado no router do Vault. Apenas admins autenticados chegam na tela de login do cofre.
- **Cerim√¥nia de Inicializa√ß√£o (Unseal):**
    - Executada inicializa√ß√£o com **Shamir's Secret Sharing**.
    - **Configura√ß√£o:** 5 Key Shares, Threshold de 3 chaves para desbloqueio.
    - **Root Token:** Gerado e armazenado com seguran√ßa m√°xima (Bitwarden, depois para o Vaultwarden) junto com as 5 chaves de unseal.
- **Estado Final:**
    - Vault operacional em `https://vault.home`.
    - Banco de dados criptografado em repouso.
    - Requer desbloqueio manual (3 chaves) a cada reinicializa√ß√£o do container.
## 2026-01-02 (Parte 4)
**Status:** ‚úÖ Sucesso (Hardening RBAC)

**Foco:** Restri√ß√£o de Acesso via Policy (Python)

- **Objetivo:** Impedir que qualquer usu√°rio logado no Authentik (mesmo sem privil√©gios) acesse o painel administrativo do Traefik. Apenas a equipe de infraestrutura deve ter acesso.
- **Implementa√ß√£o:**
    - Criado grupo `infra-admins` no Authentik e inclu√≠do o usu√°rio administrador.
    - Criada uma **Expression Policy** (Python) para validar a pertin√™ncia ao grupo:
      ```python
      return ak_is_group_member(request.user, name="infra-admins")
      ```
    - Vinculada a policy ao aplicativo `Traefik Dashboard` com prioridade 0.
- **Valida√ß√£o:**
    - Login com admin: **Sucesso** (Acesso liberado).
    - Login com usu√°rio comum: **Bloqueado** (Mensagem "Permission Denied" exibida pelo Authentik).
## 2026-01-02 (Parte 3)
**Status:** ‚úÖ Sucesso (Identity Provider & Zero Trust)

**Foco:** Implementa√ß√£o do Authentik e Integra√ß√£o com Traefik (ForwardAuth)

- **Desafio 1 (Erro Operacional):**
    - Durante a configura√ß√£o dos arquivos `docker-compose.yml`, houve uma **sobrescrita acidental** do arquivo do Authentik com o conte√∫do do Traefik. Isso causou a queda de ambos os servi√ßos.
    - *Recupera√ß√£o:* Foi necess√°rio restaurar manualmente os manifestos YAML corretos em `/opt/auth/authentik` e `/opt/traefik` e recriar os containers (`force-recreate`).
- **Desafio 2 (O Erro 404 no Callback):**
    - Ap√≥s configurar o middleware, o fluxo de login iniciava, mas falhava no retorno (`/outpost.goauthentik.io/callback...`) com erro 404 do Traefik.
    - **Causa T√©cnica:** O Traefik bloqueava a URL de callback porque ela n√£o correspondia √† regra restrita do Dashboard (`PathPrefix(/dashboard)`).
- **Solu√ß√£o Definitiva (Global Callback Route):**
    - Adicionada uma Label no servi√ßo do Authentik criando um Router dedicado: `Rule=PathPrefix(/outpost.goauthentik.io/)`.
    - Isso instrui o Traefik a interceptar *qualquer* requisi√ß√£o de callback do Authentik, independente do dom√≠nio, e encaminh√°-la para o container do IdP.
- **Resultado:**
    - Acesso a `https://traefik.home/dashboard/` redireciona para `auth.home`, exige credenciais e retorna com sucesso.
    - Porta 8080 do Traefik foi fechada definitivamente.
## 2026-01-02 (Parte 2)
**Status:** ‚úÖ Sucesso (Hardening)

**Foco:** Seguran√ßa do DockerHost e Padroniza√ß√£o

- **Motiva√ß√£o:** Antes de implementar a camada de identidade (Authentik), identifiquei que o Traefik mantinha acesso direto e irrestrito ao `docker.sock`. Isso violava o princ√≠pio do menor privil√©gio (Security by Design).
- **A√ß√µes de Mitiga√ß√£o:**
    - **Socket Proxy:** Interpus um proxy que filtra chamadas de API. Agora o Traefik s√≥ tem permiss√£o para listar containers (`GET`). Comandos destrutivos ou de cria√ß√£o (`POST`, `DELETE`) s√£o bloqueados silenciosamente.
    - **Resili√™ncia de Disco:** Configurei rota√ß√£o de logs global no Docker Daemon (Max 3 arquivos de 10MB) para evitar que servi√ßos verbosos lotem o armazenamento de 32GB.
    - **OS Patching:** Debian configurado para aplicar patches de seguran√ßa automaticamente (`unattended-upgrades`).
    - **Organiza√ß√£o:** Migrei servi√ßos dispersos para a hierarquia `/opt/services/` e padronizei o ownership para o usu√°rio comum, removendo a necessidade de operar arquivos como root.
## 2026-01-02
**Status:** ‚úÖ Sucesso Definitivo (Traefik v3.6)

**Foco:** Upgrade para Traefik v3.6 (Latest Stable) e Valida√ß√£o de Ingress

- **Decis√£o Estrat√©gica:**
    - Optado por n√£o manter a vers√£o legado (v2.11) e migrar imediatamente para **Traefik v3.6** para evitar d√≠vida t√©cnica futura (EOL em Fev/2026).
- **Implementa√ß√£o (The Fix):**
    - Configurado container `traefik:v3.6`.
    - Mantida a vari√°vel de ambiente `DOCKER_API_VERSION=1.45`.
    - **Resultado:** A biblioteca client do Traefik v3 respeitou a vari√°vel e ignorou a negocia√ß√£o de vers√£o falha, conectando-se perfeitamente ao Docker Engine do Debian 13.
- **Valida√ß√£o T√©cnica (Headers):**
    - `whoami` reportou `X-Forwarded-Proto: https` (Termina√ß√£o SSL OK).
    - `X-Real-Ip: 10.10.20.101` (Roteamento de VLANs transparente, sem mascaramento de IP).
    - Logs do Traefik limpos, sem erros de API.
## 2025-12-31
**Status:** ‚úÖ Sucesso (Traefik & Ingress)

**Foco:** Implementa√ß√£o do Proxy Reverso (Traefik) e Compatibilidade Docker API

- **Desafio (Dependency Hell):**
    - O Docker Engine no **Debian 13 (Trixie)** exige API m√≠nima `1.44`.
    - O **Traefik v3** tenta negociar vers√µes antigas (`1.24`) por padr√£o e falha em ambientes *bleeding edge*.
    - Tentativas de for√ßar a vers√£o via flags (`--providers.docker.apiVersion`) ou vari√°veis (`DOCKER_API_VERSION`) no Traefik v3 falharam silenciosamente devido a mudan√ßas recentes na lib interna.
- **Solu√ß√£o (Downgrade T√°tico):**
    - Revertido para **Traefik v2.11** (LTS).
    - Injetada vari√°vel de ambiente `DOCKER_API_VERSION=1.45` diretamente no container.
    - Isso for√ßou o cliente Docker interno do Traefik a falar a l√≠ngua do Debian 13 sem negocia√ß√£o.
- **Valida√ß√£o:**
    - Acesso a `https://whoami.home` confirmado.
    - Redirecionamento HTTP -> HTTPS (80 -> 443) ativo.
    - **Header X-Real-IP:** O container recebe o IP real do cliente (`10.10.20.x`), confirmando que o roteamento Inter-VLAN est√° transparente.
- **Observa√ß√£o:**
    - Atualizar assim que poss√≠vel para a vers√£o mais recente (v2.11 ends Feb 01, 2026).
## 2025-12-30
**Status:** ‚úÖ Sucesso (DNS & Privacy)

**Foco:** Implementa√ß√£o do AdGuard Home e Gest√£o de DNS
- **Infraestrutura DNS (LXC Container):**
    - Criado Container LXC `101 (AdGuard-Primary)` baseado em Alpine Linux (3.23) na VLAN 30.
    - **Specs:** 1 Core, 256MB RAM, IP Est√°tico `10.10.30.5`.
    - **Software:** AdGuard Home instalado via script oficial.
        - `curl -s -S -L https://raw.githubusercontent.com/AdguardTeam/AdGuardHome/master/scripts/install.sh | sh -s -- -v`, dispon√≠vel [aqui](https://github.com/AdguardTeam/AdGuardHome
).
- **Configura√ß√£o do Servi√ßo (AdGuard):**
    - **Upstreams:** Configurados servidores DNS-over-HTTPS (Cloudflare/Quad9) para garantir privacidade e evitar intercepta√ß√£o de porta 53 pelo ISP.
    - **Reverse DNS:** Apontado para o OPNsense (`10.10.30.1`) para resolu√ß√£o correta de hostnames locais nos logs.
- **Integra√ß√£o de Rede (OPNsense DHCP):**
    - Alterado o servidor DNS entregue via DHCP para as VLANs **TRUSTED (20)** e **IOT (50)**:
        - **De:** `1.1.1.1` e `8.8.8.8` (Externos (Cloudflare e Google, respectivamente).
        - **Para:** `10.10.30.5` (Local (AdGuard)).
    - **Pol√≠tica de Resili√™ncia:** A VLAN **SERVER (30)** teve seu DNS mantido em `1.1.1.1` para evitar depend√™ncia c√≠clica (o DockerHost n√£o deve depender de um container vizinho para resolver nomes durante o boot).
- **Valida√ß√£o:**
    - Cliente Arch Linux (VLAN 20) renovou DHCP e confirmou recebimento do DNS `10.10.30.5` via `/etc/resolv.conf`.
    - Dashboard do AdGuard registrou queries vindas da rede TRUSTED e bloqueios ativos.
    - O mesmo foi realizado com a VLAN 50.
- Documenta√ß√£o do repo melhor documentada e formatada.
- Repo aberto.
## 2025-12-29
**Status:** ‚úÖ Sucesso (Docker & Hardening)

**Foco:** Configura√ß√£o do DockerHost e Ajuste de Firewall

- **Hardening SSH:**
    - Chaves Ed25519 copiadas do Arch Linux para o DockerHost.
    - **Configura√ß√£o de Seguran√ßa:** Editado `/etc/ssh/sshd_config` para:
        * `PermitRootLogin no` (Bloqueio total de login direto como root via SSH).
        * `PasswordAuthentication no` (Autentica√ß√£o por senha desativada; apenas chaves SSH).
        * `PubkeyAuthentication yes` (Autentica√ß√£o por chave p√∫blica habilitada).
        * `ChallengeResponseAuthentication no` (Desativa m√©todos interativos/legados de autentica√ß√£o).
        * `UsePAM yes` (Mant√©m PAM ativo para controle de sess√£o e pol√≠ticas do sistema).
    - **Valida√ß√£o:** Login verificado com sucesso via chave; tentativa de login por senha rejeitada como esperado.

- **Instala√ß√£o do Docker:**
    - Utilizado reposit√≥rio oficial (m√©todo compat√≠vel com Debian Trixie/Bookworm).
    - Engine e Plugin Compose (v5.0.0) instalados.
    - Usu√°rio adicionado ao grupo `docker` para execu√ß√£o sem root/sudo.
    - **Teste de Sanidade:** `docker run hello-world` executado com sucesso (Pull da imagem via WAN OK, Execu√ß√£o OK).

- **Incidente de Conectividade (Firewall):**
    - *Sintoma:* O Arch Linux (VLAN 20) n√£o conseguia pingar ou conectar via SSH no DockerHost (VLAN 30), resultando em Timeout.
    - *Causa Raiz:* Esquecimento da pol√≠tica de "Default Deny". Embora a VLAN 30 tivesse permiss√£o de sa√≠da (para internet), a VLAN 20 n√£o tinha permiss√£o expl√≠cita de **entrada/passagem** para a VLAN 30.
    - *Solu√ß√£o:* Criada regra de Firewall na interface **TRUSTED**:
        - **Action:** Pass
        - **Source:** TRUSTED net
        - **Destination:** Any (ou SERVER net)
        - **Justificativa:** Permite que dispositivos de gerenciamento acessem os servidores.
        - O mesmo foi feito com a VLAN 50 (IOT).
## 2025-12-28
**Status:** ‚ö†Ô∏è Resgate de Rede (Driver Migration)

**Foco:** Recupera√ß√£o das VLANs ap√≥s mudan√ßa para VirtIO

- **O Incidente:**
    - Ao verificar a VM `DockerHost`, notei que ela n√£o pegava IP (estava com APIPA `169.254.x.x`).
    - No OPNsense, as interfaces **TRUSTED**, **SERVER** e **IOT** haviam desaparecido do painel de controle, restando apenas LAN e WAN.
- **Diagn√≥stico:**
    - A mudan√ßa do driver de rede da VM OPNsense (de `e1000` para `VirtIO`) alterou a nomenclatura das interfaces no BSD (de `em0` para `vtnet0/1`).
    - Isso quebrou a associa√ß√£o "Parent Interface" das VLANs, tornando-as √≥rf√£s e desativadas.
    - Identifiquei via MAC Address (`04:FD`) que a interface `vtnet1` (atualmente WAN) era, na verdade, a porta f√≠sica configurada com Trunks no Proxmox.
- **Solu√ß√£o:**
    1. **Reparenting:** Reconfigurei as VLANs 20, 30 e 50 para usarem a interface correta (`vtnet1`) como pai.
    2. **Re-assignment:** Re-adicionei as interfaces l√≥gicas que haviam sumido.
    3. **Re-IP:** Restaurei os IPs Est√°ticos (`10.10.x.1`) e servi√ßos DHCP que foram limpos durante a falha.
- **Resultado:** A VM DockerHost obteve o IP `10.10.30.102` imediatamente ap√≥s o fix.
## 2025-12-27
**Status:** ‚úÖ Sucesso

**Foco:** Provisionamento do DockerHost e Segmenta√ß√£o VLAN 30

- **Infraestrutura de Rede (VLAN 30 - SERVER):**
    - Configurada interface l√≥gica no OPNsense (`10.10.30.1/24`) com DHCP ativado (`.100` a `.200`).
    - Validado isolamento: `ping` da VLAN 20 (Trusted) para 50 (IoT) falha como esperado (Bloqueio padr√£o).
    - Regras de Firewall: Criada regra tempor√°ria "Pass All" na VLAN 30 para permitir instala√ß√£o de pacotes.
- **Computa√ß√£o (VM DockerHost):**
    - Criada VM ID `105` (Debian 13 Minimal (somente com SSH Server e Standard system utilities)).
    - **Specs:** 2 vCores (Host), 8GB RAM (Static), 32GB Disk (VirtIO Block).
    - **Rede:** Interface VirtIO com **Tag 30** definida no Proxmox.
    - **Valida√ß√£o:**
        - VM obteve IP `10.10.30.x` automaticamente.
        - Conectividade externa (WAN) funcionando via NAT H√≠brido.
        - Acesso SSH verificado a partir da VLAN 20 (Trusted).
## 2025-12-26
**Status:** ‚úÖ Sucesso Cr√≠tico (Rede Funcional)

**Foco:** Troubleshooting de VLANs, Switch e Roteamento OPNsense

- **O Incidente:** O DHCP n√£o chegava aos clientes via Wi-Fi (VLANs 20/50) e, quando chegava (ap√≥s fix), n√£o havia navega√ß√£o.
- **Diagn√≥stico e Solu√ß√µes (Post-Mortem):**
    1. **Proxmox Bridge Dropping Tags:** A bridge `vmbr0` (VLAN Aware) estava descartando pacotes taggeados (20, 50) antes de entreg√°-los √† VM.
        - *Corre√ß√£o:* Adicionado `bridge-vids 2-4094` em `/etc/network/interfaces` no Host.
        - *Corre√ß√£o:* Adicionado `trunks=20;50` na configura√ß√£o da interface de rede da VM (`/etc/pve/qemu-server/100.conf`).
    2. **Conflito de Roteamento (Routing Loop):** A interface LAN (`192.168.0.250/24`) e WAN (`192.168.0.50/24`) estavam na mesma sub-rede. O kernel do OPNsense entrava em conflito de rota ao tentar responder a pacotes de outras VLANs, causando erro *"Provide a valid source address"* no Ping.
        - *Solu√ß√£o Definitiva:* Alterado IP da LAN para `192.168.99.1/24` para isolar as redes.
    3. **Hardware Offloading (VirtIO):** Pacotes DHCP chegavam corrompidos/descartados.
        - *Ajuste:* Desativado Hardware CRC, TSO e LRO nas configura√ß√µes do OPNsense.
    4. **Firewall Block:** VLANs novas v√™m com "Default Deny".
        - *Ajuste:* Criadas regras de "Pass All" e configurado Outbound NAT H√≠brido.
## 2025-12-25
**Status:** üîÑ Troca de Hardware

**Foco:** Aquisi√ß√£o de Storage para Bitcoin Node

- **Problema Log√≠stico:** O SSD SanDisk (comprado em 14/12) entrou em estado de atraso indefinido no Mercado Livre ("Em prepara√ß√£o" por 10 dias). Compra cancelada para evitar parada no projeto.
- **Revis√£o T√©cnica:** Aproveitei o incidente para reavaliar a especifica√ß√£o. Identifiquei que o SanDisk Plus √© **DRAM-less**. Para um Full Node Bitcoin, isso seria catastr√≥fico durante o IBD (Initial Block Download), pois o esgotamento do cache SLC derrubaria a velocidade de escrita drasticamente.
- **Decis√£o:** Adquirido **Samsung 870 EVO 2TB** (Envio Full).
    - Embora o custo seja marginalmente maior, ele possui **2GB de Cache LPDDR4** e controlador MKX. Isso garante que a sincroniza√ß√£o da blockchain ocorra na velocidade m√°xima da interface SATA, economizando dias de espera futura.
    - A placa de rede HP NC364T (incompat√≠vel) devolvida tamb√©m serviu para abater a diferen√ßa de custo.
## 2025-12-24
**Status:** ‚ö†Ô∏è Resgate de Rede (Rollback)

**Foco:** Recupera√ß√£o de Acesso e Simplifica√ß√£o de Rede

- **O Incidente:**
    - Ap√≥s o sucesso inicial com o Dropbear, tentamos migrar para a topologia "Router-on-a-Stick" configurando VLANs (10, 20, 90) no OPNsense e no Switch.
    - **Resultado:** Perda total de acesso (Lockout). O Dropbear parou de responder e o Proxmox ficou inacess√≠vel.
- **Diagn√≥stico (A Causa Raiz):**
    1. **Hardcoding no Boot:** O arquivo `/etc/initramfs-tools/initramfs.conf` continha uma linha for√ßando IP Est√°tico (`IP:10.10.10.1...`).
    2. **Desalinhamento:** O Switch foi configurado para esperar VLANs, mas o servidor bootava for√ßando um IP fora da sub-rede e sem tagging, causando falha de comunica√ß√£o.
- **A Solu√ß√£o (O Resgate):**
    - **Physical Reset:** Reset f√≠sico do Switch TP-Link para configura√ß√µes de f√°brica (Rede Flat 192.168.0.x).
    - **Boot Config:** Editado `initramfs.conf` para remover o IP est√°tico e definir `IP=dhcp`.
    - **Proxmox Config:** Editado `/etc/network/interfaces` para usar DHCP na `vmbr0`.
- **Li√ß√£o Aprendida:**
    - **NUNCA** definir IPs est√°ticos no `initramfs` em ambiente de Homelab. Usar `IP=dhcp` e controlar a fixa√ß√£o de IP via reserva no Roteador (DHCP Static Lease).
    - O Dropbear (Desbloqueio) deve permanecer sempre na VLAN Nativa/Untagged (Rede "Burra") para garantir acesso de emerg√™ncia independente do estado do OPNsense.
## 2025-12-22
**Status:** ‚úÖ Sucesso Total

**Foco:** Otimiza√ß√£o de Hardware e Router-on-a-Stick

- **Decis√£o T√©cnica:** A placa HP Quad-Port foi removida. O custo de complexidade de driver e energia n√£o justificava o uso, dado que o switch TP-Link gerencia VLANs com perfei√ß√£o.
- **Troubleshooting Dropbear:** Ap√≥s a remo√ß√£o da placa HP, o nome da interface mudou de `enp8s0` para `enp4s0`. Isso quebrou o desbloqueio remoto inicial.
    - *Corre√ß√£o:* Atualizei o `initramfs.conf` com `DEVICE=enp4s0` e fixei a porta `2222`. O teste de `cryptroot-unlock` via SSH no notebook Arch funcionou ap√≥s limpar o `known_hosts`.
- **OPNsense:** WAN configurada com sucesso na VLAN 90. O IP foi obtido via DHCP do modem em modo DMZ.
## 2025-12-21
**Status:** ‚úÖ Sucesso

**Foco:** Criptografia (FDE), Swap e Desbloqueio Remoto

- **LUKS:** Realizei a convers√£o p√≥s-instala√ß√£o do Proxmox para **LUKS2** (Full Disk Encryption) seguindo o guia manual. 
- **Swap:** Configurei um **ZFS Swap de 16GB** para evitar travamentos por exaust√£o de mem√≥ria (OOM), j√° que o ZFS sem swap pode entrar em deadlock.
- **Dropbear:** Configurei o servidor SSH leve (Dropbear) no initramfs.
    - **Teste:** Reiniciei o servidor sem monitor. Conectei via SSH na porta tempor√°ria, digitei a senha do disco e o boot do Proxmox prosseguiu corretamente.

## 2025-12-20
**Status:** ‚úÖ Sucesso

**Foco:** Dry Run (Instala√ß√£o e Rede)

- **Instala√ß√£o Base:** Instalei o Proxmox VE 9.1 para validar a detec√ß√£o de hardware.
- **Rede:**
    - A interface Onboard foi identificada como `eno1` (Driver `r8169`).
    - A placa HP Quad-Port foi identificada corretamente (Driver `e1000e`).
    - **Lat√™ncia:** Teste de ping direto registrou `0.2ms`.
- **Armazenamento:** O **ZFS Mirror (RAID 1)** foi montado e ativado no `rpool` com os dois NVMe Kingston.
- **Troubleshooting:** Tive dificuldade inicial para pingar o servidor (10.10.10.x) a partir do meu Arch Linux.
    - *Solu√ß√£o:* Era necess√°rio ajustar as regras de entrada/sa√≠da no firewall do cliente (Arch), pois n√£o h√° roteador intermediando a conex√£o f√≠sica direta neste est√°gio.

## 2025-12-19
**Status:** ‚úÖ Sucesso

**Foco:** Hardware Burn-in e BIOS

- **Valida√ß√£o de Mem√≥ria:** Executei o **MemTest86 V11.5** por 6 horas e 17 minutos.
    - **Resultado:** 48/48 testes completados com **0 Erros**.
    - *Telemetria:* XMP validado a 3192 MT/s. A temperatura m√°xima da CPU ficou em 48¬∞C, validando a instala√ß√£o do cooler AK400.
![Evid√™ncia do MemTest86](../assets/benchmarks/MemTest86.jpeg)
- **Configura√ß√£o da BIOS:** Apliquei as configura√ß√µes cr√≠ticas na Gigabyte B760M.
